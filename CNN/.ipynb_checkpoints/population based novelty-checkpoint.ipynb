{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######code for directly calculate distance between stored architectures(last 1000)\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import genotypes\n",
    "from model_search import Network\n",
    "import utils\n",
    "\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "sys.path.append('/data/mzhang3/randomNAS_release-master')\n",
    "import shutil\n",
    "import inspect\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "from numpy.linalg import cholesky\n",
    "\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "class DartsWrapper:\n",
    "    def __init__(self, save_path, seed, batch_size, grad_clip, epochs, resume_iter=None, init_channels=16):\n",
    "        args = {}\n",
    "        args['data'] = '/data/mzhang3/randomNAS_own/data'\n",
    "        args['epochs'] = epochs\n",
    "        args['learning_rate'] = 0.025\n",
    "        args['batch_size'] = batch_size\n",
    "        args['learning_rate_min'] = 0.001\n",
    "        args['momentum'] = 0.9\n",
    "        args['weight_decay'] = 3e-4\n",
    "        args['init_channels'] = init_channels\n",
    "        args['layers'] = 8\n",
    "        args['drop_path_prob'] = 0.3\n",
    "        args['grad_clip'] = grad_clip\n",
    "        args['train_portion'] = 0.5\n",
    "        args['seed'] = seed\n",
    "        args['log_interval'] = 50\n",
    "        args['save'] = save_path\n",
    "        args['gpu'] = 0\n",
    "        args['cuda'] = True\n",
    "        args['cutout'] = False\n",
    "        args['cutout_length'] = 16\n",
    "        args['report_freq'] = 50\n",
    "        args = AttrDict(args)\n",
    "        self.args = args\n",
    "        self.seed = seed\n",
    "\n",
    "        np.random.seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.enabled=True\n",
    "        cudnn.deterministic=True\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "        train_transform, valid_transform = utils._data_transforms_cifar10(args)\n",
    "        train_data = dset.CIFAR10(root=args.data, train=True, download=False, transform=train_transform)\n",
    "\n",
    "        num_train = len(train_data)\n",
    "        indices = list(range(num_train))\n",
    "        split = int(np.floor(args.train_portion * num_train))\n",
    "\n",
    "        self.train_queue = torch.utils.data.DataLoader(\n",
    "          train_data, batch_size=args.batch_size,\n",
    "          sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "          pin_memory=True, num_workers=0, worker_init_fn=np.random.seed(args.seed))\n",
    "\n",
    "        self.valid_queue = torch.utils.data.DataLoader(\n",
    "          train_data, batch_size=args.batch_size,\n",
    "          sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "          pin_memory=True, num_workers=0, worker_init_fn=np.random.seed(args.seed))\n",
    "\n",
    "        self.train_iter = iter(self.train_queue)\n",
    "        self.valid_iter = iter(self.valid_queue)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.epochs = 0\n",
    "        self.total_loss = 0\n",
    "        self.start_time = time.time()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        criterion = criterion.cuda()\n",
    "        self.criterion = criterion\n",
    "\n",
    "        model = Network(args.init_channels, 10, args.layers, self.criterion)\n",
    "\n",
    "        model = model.cuda()\n",
    "        self.model = model\n",
    "\n",
    "     #   try:\n",
    "        #    self.load()\n",
    "      #      logging.info('loaded previously saved weights')\n",
    "      #  except Exception as e:\n",
    "      #      print(e)\n",
    "\n",
    "        logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\n",
    "\n",
    "        optimizer = torch.optim.SGD(\n",
    "          self.model.parameters(),\n",
    "          args.learning_rate,\n",
    "          momentum=args.momentum,\n",
    "          weight_decay=args.weight_decay)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "          optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
    "\n",
    "        if resume_iter is not None:\n",
    "            self.steps = resume_iter\n",
    "            self.epochs = int(resume_iter / len(self.train_queue))\n",
    "            logging.info(\"Resuming from epoch %d\" % self.epochs)\n",
    "            self.objs = utils.AvgrageMeter()\n",
    "            self.top1 = utils.AvgrageMeter()\n",
    "            self.top5 = utils.AvgrageMeter()\n",
    "            for i in range(self.epochs):\n",
    "                self.scheduler.step()\n",
    "\n",
    "        size = 0\n",
    "        for p in model.parameters():\n",
    "            size += p.nelement()\n",
    "        logging.info('param size: {}'.format(size))\n",
    "\n",
    "        total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "        logging.info('Args: {}'.format(args))\n",
    "        logging.info('Model total parameters: {}'.format(total_params))\n",
    "\n",
    "    def train_batch(self, arch):\n",
    "        args = self.args\n",
    "        if self.steps % len(self.train_queue) == 0:\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            self.objs = utils.AvgrageMeter()\n",
    "            self.top1 = utils.AvgrageMeter()\n",
    "            self.top5 = utils.AvgrageMeter()\n",
    "        lr = self.scheduler.get_lr()[0]\n",
    "\n",
    "        weights = self.get_weights_from_arch(arch)\n",
    "        self.set_model_weights(weights)\n",
    "\n",
    "        step = self.steps % len(self.train_queue)\n",
    "        input, target = next(self.train_iter)\n",
    "\n",
    "        self.model.train()\n",
    "        n = input.size(0)\n",
    "\n",
    "        input = Variable(input, requires_grad=False).cuda()\n",
    "        target = Variable(target, requires_grad=False).cuda(async=True)\n",
    "\n",
    "      # get a random minibatch from the search queue with replacement\n",
    "        self.optimizer.zero_grad()\n",
    "        logits = self.model(input)\n",
    "        loss = self.criterion(logits, target)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.model.parameters(), args.grad_clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        self.objs.update(loss.data, n)\n",
    "        self.top1.update(prec1.data, n)\n",
    "        self.top5.update(prec5.data, n)\n",
    "\n",
    "        if step % args.report_freq == 0:\n",
    "            logging.info('train %03d %e %f %f', step, self.objs.avg, self.top1.avg, self.top5.avg)\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps % len(self.train_queue) == 0:\n",
    "            self.epochs += 1\n",
    "            self.train_iter = iter(self.train_queue)\n",
    "            valid_err = self.evaluate(arch)\n",
    "            logging.info('epoch %d  |  train_acc %f  |  valid_acc %f' % (self.epochs, self.top1.avg, 1-valid_err))\n",
    "            self.save()\n",
    "\n",
    "    def evaluate(self, arch, split=None):\n",
    "      # Return error since we want to minimize obj val\n",
    "        logging.info(arch)\n",
    "        objs = utils.AvgrageMeter()\n",
    "        top1 = utils.AvgrageMeter()\n",
    "        top5 = utils.AvgrageMeter()\n",
    "\n",
    "        weights = self.get_weights_from_arch(arch)\n",
    "        self.set_model_weights(weights)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        if split is None:\n",
    "            n_batches = 10\n",
    "        else:\n",
    "            n_batches = len(self.valid_queue)\n",
    "\n",
    "        for step in range(n_batches):\n",
    "            try:\n",
    "                input, target = next(self.valid_iter)\n",
    "            except Exception as e:\n",
    "                logging.info('looping back over valid set')\n",
    "                self.valid_iter = iter(self.valid_queue)\n",
    "                input, target = next(self.valid_iter)\n",
    "            input = Variable(input).cuda()\n",
    "            target = Variable(target).cuda(async=True)\n",
    "\n",
    "            logits = self.model(input)\n",
    "            loss = self.criterion(logits, target)\n",
    "\n",
    "            prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "            n = input.size(0)\n",
    "            objs.update(loss.data, n)\n",
    "            top1.update(prec1.data, n)\n",
    "            top5.update(prec5.data, n)\n",
    "\n",
    "            if step % self.args.report_freq == 0:\n",
    "                logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "\n",
    "        return 1-(top1.avg)/100\n",
    "\n",
    "    def save(self):\n",
    "        utils.save(self.model, os.path.join(self.args.save, 'weights.pt'))\n",
    "\n",
    "    def load(self):\n",
    "        utils.load(self.model, os.path.join(self.args.save, 'weights.pt'))\n",
    "\n",
    "    def get_weights_from_arch(self, arch):\n",
    "        k = sum(1 for i in range(self.model._steps) for n in range(2+i))\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "        n_nodes = self.model._steps\n",
    "\n",
    "        alphas_normal = Variable(torch.zeros(k, num_ops).cuda(), requires_grad=False)\n",
    "        alphas_reduce = Variable(torch.zeros(k, num_ops).cuda(), requires_grad=False)\n",
    "\n",
    "        offset = 0\n",
    "        for i in range(n_nodes):\n",
    "            normal1 = arch[0][2*i]\n",
    "            normal2 = arch[0][2*i+1]\n",
    "            reduce1 = arch[1][2*i]\n",
    "            reduce2 = arch[1][2*i+1]\n",
    "            alphas_normal[offset+normal1[0], normal1[1]] = 1\n",
    "            alphas_normal[offset+normal2[0], normal2[1]] = 1\n",
    "            alphas_reduce[offset+reduce1[0], reduce1[1]] = 1\n",
    "            alphas_reduce[offset+reduce2[0], reduce2[1]] = 1\n",
    "            offset += (i+2)\n",
    "\n",
    "        arch_parameters = [\n",
    "          alphas_normal,\n",
    "          alphas_reduce,\n",
    "        ]\n",
    "        return arch_parameters\n",
    "\n",
    "    def set_model_weights(self, weights):\n",
    "        self.model.alphas_normal = weights[0]\n",
    "        self.model.alphas_reduce = weights[1]\n",
    "        self.model._arch_parameters = [self.model.alphas_normal, self.model.alphas_reduce]\n",
    "        \n",
    "\n",
    "    def novelty_fitness(self,arch,store_arch,k):\n",
    "\n",
    "        def mapping(sample_arch):\n",
    "            arch_map = np.zeros((14,7))\n",
    "            for i in range(8):\n",
    "                ind_node=i//2\n",
    "                k = sum(1 for i in range(ind_node) for n in range(2+i))\n",
    "                ind_nomal_op=np.int(sample_arch[2*i+1])\n",
    "                ind_nomal_con=np.int(k+sample_arch[2*i])\n",
    "                arch_map[ind_nomal_con,ind_nomal_op]=arch_map[ind_nomal_con,ind_nomal_op]+1\n",
    "            return arch_map\n",
    "\n",
    "        def cal_vec_dis(vec1,vec2):\n",
    "            if  np.linalg.norm(vec1-vec2)==0:\n",
    "                dis=0\n",
    "            else:\n",
    "                dis=1\n",
    "            return dis\n",
    "\n",
    "        def dis_arch(arch1,arch2):\n",
    "            mat1=mapping(arch1)\n",
    "            mat2=mapping(arch2)\n",
    "            dis=0\n",
    "            for i in range(mat1.shape[0]):\n",
    "                dis=dis+cal_vec_dis(mat1[i,],mat2[i,])\n",
    "            dis=dis/mat1.shape[0]\n",
    "            return dis\n",
    "        \n",
    "        store_arch=store_arch[:-1000,]\n",
    "        \n",
    "        n_connec=2*self.model._steps              \n",
    "        arch=np.array(arch)        \n",
    "        arch=np.reshape(arch,(1,n_connec*2))\n",
    "\n",
    "        dis=np.zeros((store_arch.shape[0],))\n",
    "        for i in range(store_arch.shape[0]):\n",
    "            dis[i]=dis_arch(arch[0,],store_arch[i,])\n",
    "        sort_dis=np.sort(dis)\n",
    "        novelty_dis=np.mean(sort_dis[0:k])\n",
    "        \n",
    "        return novelty_dis      \n",
    "        \n",
    "        \n",
    "\n",
    "    def sample_arch_eval(self):\n",
    "        k = sum(1 for i in range(self.model._steps) for n in range(2+i))\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "        n_nodes = self.model._steps\n",
    "\n",
    "        normal = []\n",
    "        reduction = []\n",
    "        for i in range(n_nodes):\n",
    "            ops = np.random.choice(range(num_ops), 4)\n",
    "            nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)\n",
    "            nodes_in_reduce = np.random.choice(range(i+2), 2, replace=False)\n",
    "            normal.extend([(nodes_in_normal[0], ops[0]), (nodes_in_normal[1], ops[1])])\n",
    "            reduction.extend([(nodes_in_reduce[0], ops[2]), (nodes_in_reduce[1], ops[3])])\n",
    "        return (normal, reduction)    \n",
    "        \n",
    "    def sample_arch(self,node_id,store_normal_arch,store_reduce_arch):\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "        num_nodes=self.model._steps\n",
    "        \n",
    "            \n",
    "        def limite_range(arch,num_ops,num_nodes):\n",
    "            for i in range(num_nodes):\n",
    "                arch[4*i,]=np.max((np.min((arch[4*i,],(i+1))),0))\n",
    "                arch[4*i+1,]=np.max((np.min((arch[4*i+1,],num_ops)),0))\n",
    "                arch[4*i+2,]=np.max((np.min((arch[4*i+2,],(i+1))),0))\n",
    "                arch[4*i+3,]=np.max((np.min((arch[4*i+3,],num_ops)),0))\n",
    "            return arch       \n",
    "       \n",
    "        if node_id>999:\n",
    "            alfa=0.1\n",
    "            n=10\n",
    "            sigma=1\n",
    "\n",
    "            mu=np.zeros((1,4*self.model._steps))\n",
    "            Sigma=np.eye(4*self.model._steps)\n",
    "            R=cholesky(Sigma)\n",
    "            \n",
    "            \n",
    "            yita=np.dot(np.random.randn(n,4*self.model._steps),R)+mu\n",
    "            n_yita=np.empty((n,4*self.model._steps))\n",
    "                        \n",
    "            index0=np.random.randint(1000)\n",
    "            test_normal_arch=store_normal_arch[index0,]\n",
    "            \n",
    "            for i in range(n):\n",
    "                n_f=self.novelty_fitness(np.int_(np.round((test_normal_arch+yita[i,]))),np.int_(np.round(store_normal_arch)),10)\n",
    "                n_yita[i,]=n_f*yita[i,]\n",
    "            selec_normal=test_normal_arch+alfa*(1/(n*sigma))*sum(n_yita)\n",
    "            store_normal_arch[index0,]=selec_normal             \n",
    "            selec_normal=np.int_(np.round(selec_normal))            \n",
    "            selec_normal=limite_range(selec_normal,num_ops,num_nodes)\n",
    "            \n",
    "            \n",
    "            \n",
    "            yita=np.dot(np.random.randn(n,4*self.model._steps),R)+mu\n",
    "            n_yita=np.empty((n,4*self.model._steps))\n",
    "            \n",
    "            index1=np.random.randint(1000)\n",
    "            test_reduce_arch=store_reduce_arch[index1,]\n",
    "            \n",
    "            \n",
    "          \n",
    "            for i in range(n):\n",
    "                n_f=self.novelty_fitness(np.int_(np.round((test_reduce_arch+yita[i,]))),np.int_(np.round(store_reduce_arch)),10)\n",
    "                n_yita[i,]=n_f*yita[i,]\n",
    "            selec_reduce=test_reduce_arch+alfa*(1/(n*sigma))*sum(n_yita)\n",
    "            store_reduce_arch[index1,]=selec_reduce      \n",
    "            selec_reduce=np.int_(np.round(selec_reduce))                \n",
    "            selec_reduce=limite_range(selec_reduce,num_ops,num_nodes)\n",
    "            \n",
    "          \n",
    "            normal = []\n",
    "            reduction = []\n",
    "            for i in range(self.model._steps):\n",
    "                s1=np.int(selec_normal[4*i,])\n",
    "                s2=np.int(selec_normal[4*i+1,])\n",
    "                s3=np.int(selec_normal[4*i+2,])\n",
    "                s4=np.int(selec_normal[4*i+3,])\n",
    "                s5=np.int(selec_reduce[4*i,])\n",
    "                s6=np.int(selec_reduce[4*i+1,])\n",
    "                s7=np.int(selec_reduce[4*i+2,])\n",
    "                s8=np.int(selec_reduce[4*i+3,])\n",
    "                normal.extend([(s1,s2), (s3,s4)])\n",
    "                reduction.extend([(s5,s6), (s7,s8)]) \n",
    "            index=(index0,index1)\n",
    "                                                   \n",
    "        else:     \n",
    "            k = sum(1 for i in range(self.model._steps) for n in range(2+i))\n",
    "            num_ops = len(genotypes.PRIMITIVES)\n",
    "            n_nodes = self.model._steps\n",
    "\n",
    "            normal = []\n",
    "            reduction = []\n",
    "            for i in range(n_nodes):\n",
    "                ops = np.random.choice(range(num_ops), 4)\n",
    "                nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)                \n",
    "                nodes_in_reduce = np.random.choice(range(i+2), 2, replace=False)                \n",
    "                normal.extend([(nodes_in_normal[0], ops[0]), (nodes_in_normal[1], ops[1])])\n",
    "                \n",
    "                reduction.extend([(nodes_in_reduce[0], ops[2]), (nodes_in_reduce[1], ops[3])])\n",
    "                \n",
    "            normal=np.int_(normal)\n",
    "            reduction=np.int_(reduction)\n",
    "            index=(node_id,node_id)\n",
    "######the operations from two previous node are different\n",
    "        return index, (normal, reduction)\n",
    "\n",
    "\n",
    "    def perturb_arch(self, arch):\n",
    "        new_arch = copy.deepcopy(arch)\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "\n",
    "        cell_ind = np.random.choice(2)\n",
    "        step_ind = np.random.choice(self.model._steps)\n",
    "        nodes_in = np.random.choice(step_ind+2, 2, replace=False)\n",
    "        ops = np.random.choice(range(num_ops), 2)\n",
    "\n",
    "        new_arch[cell_ind][2*step_ind] = (nodes_in[0], ops[0])\n",
    "        new_arch[cell_ind][2*step_ind+1] = (nodes_in[1], ops[1])\n",
    "        return new_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rung:\n",
    "    def __init__(self, rung, nodes):\n",
    "        self.parents = set()\n",
    "        self.children = set()\n",
    "        self.rung = rung\n",
    "        for node in nodes:\n",
    "            n = nodes[node]\n",
    "            if n.rung == self.rung:\n",
    "                self.parents.add(n.parent)\n",
    "                self.children.add(n.node_id)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, parent, arch, node_id, rung):\n",
    "        self.parent = parent\n",
    "        self.arch = arch\n",
    "        self.node_id = node_id\n",
    "        self.rung = rung\n",
    "     #  self.objective_val = self.model.evaluate(arch)  \n",
    "    def to_dict(self):\n",
    "        out = {'parent':self.parent, 'arch': self.arch, 'node_id': self.node_id, 'rung': self.rung}\n",
    "        if hasattr(self, 'objective_val'):\n",
    "            out['objective_val'] = self.objective_val\n",
    "        return out\n",
    "\n",
    "class Random_NAS:\n",
    "    def __init__(self, B, model, seed, save_dir):\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.B = B\n",
    "        self.model = model\n",
    "        self.seed = seed\n",
    "        self.iters = 0\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "        k = sum(1 for i in range(self.model.model._steps) for n in range(2+i))\n",
    "\n",
    "        self.arms = {}\n",
    "        size_arch=self.model.model._steps*4\n",
    "               \n",
    "        self.store_normal_arch=np.empty((1000,size_arch))\n",
    "        self.store_reduce_arch=np.zeros((1000,size_arch))\n",
    "        \n",
    "        \n",
    "        self.node_id = 0\n",
    "\n",
    "    def print_summary(self):\n",
    "        logging.info(self.parents)\n",
    "        objective_vals = [(n,self.arms[n].objective_val) for n in self.arms if hasattr(self.arms[n],'objective_val')]\n",
    "        objective_vals = sorted(objective_vals,key=lambda x:x[1])\n",
    "        best_arm = self.arms[objective_vals[0][0]]\n",
    "        val_ppl = self.model.evaluate(best_arm.arch, split='valid')\n",
    "        logging.info(objective_vals)\n",
    "        logging.info('best valid ppl: %.2f' % val_ppl)\n",
    "\n",
    "\n",
    "    def get_arch(self):####need to generate architecture based on novelty    \n",
    "        inde, arch = self.model.sample_arch(self.node_id,self.store_normal_arch,self.store_reduce_arch)\n",
    "        normal_arch=np.array(arch[0])\n",
    "        reduce_arch=np.array(arch[1])\n",
    "        \n",
    "        gene_len=self.model.model._steps*4\n",
    "        num_con=self.model.model._steps*2\n",
    "        \n",
    "        normal_arch=np.reshape(normal_arch,(1,gene_len))\n",
    "        reduce_arch=np.reshape(reduce_arch,(1,gene_len))\n",
    "        self.store_normal_arch[inde[0],]=normal_arch[0,]\n",
    "        self.store_reduce_arch[inde[1],]=reduce_arch[0,]\n",
    "        \n",
    "        self.arms[self.node_id] = Node(self.node_id, arch, self.node_id, 0)\n",
    "\n",
    "        self.node_id += 1\n",
    "        return arch\n",
    "\n",
    "    def save(self):\n",
    "        to_save = {a: self.arms[a].to_dict() for a in self.arms}\n",
    "        # Only replace file if save successful so don't lose results of last pickle save\n",
    "        with open(os.path.join(self.save_dir,'results_tmp.pkl'),'wb') as f:\n",
    "            pickle.dump(to_save, f)\n",
    "        shutil.copyfile(os.path.join(self.save_dir, 'results_tmp.pkl'), os.path.join(self.save_dir, 'results.pkl'))\n",
    "\n",
    "        self.model.save()\n",
    "\n",
    "    def run(self):\n",
    "        while self.iters < self.B:\n",
    "            arch = self.get_arch()#######################need to generate architecture based on novelty\n",
    "            self.model.train_batch(arch)\n",
    "            self.iters += 1\n",
    "\n",
    "            if self.iters % 500 == 0:\n",
    "                self.save()\n",
    "        self.save()\n",
    "\n",
    "              \n",
    "        \n",
    "    def get_eval_arch(self, rounds=None):\n",
    "        #n_rounds = int(self.B / 7 / 1000)\n",
    "        if rounds is None:\n",
    "            n_rounds = max(1,int(self.B/10000))\n",
    "        else:\n",
    "            n_rounds = rounds\n",
    "        best_rounds = []\n",
    "        for r in range(n_rounds):\n",
    "            sample_vals = []\n",
    "            for _ in range(1000):\n",
    "                arch = self.model.sample_arch_eval()\n",
    "                try:\n",
    "                    a=time.perf_counter()\n",
    "                    ppl = self.model.evaluate(arch) \n",
    "                    torch.cuda.synchronize()\n",
    "                    b=time.perf_counter()\n",
    "                    print(b-a)\n",
    "                    print('kkkkkkkkkkkkkkkkkk')\n",
    "                except Exception as e:\n",
    "                    ppl = 1000000\n",
    "                logging.info(arch)\n",
    "                logging.info('objective_val: %.3f' % ppl)\n",
    "                sample_vals.append((arch, ppl))\n",
    "            sample_vals = sorted(sample_vals, key=lambda x:x[1])\n",
    "\n",
    "            full_vals = []\n",
    "            if 'split' in inspect.getargspec(self.model.evaluate).args:\n",
    "                for i in range(10):\n",
    "                    arch = sample_vals[i][0]\n",
    "                    try:\n",
    "                        ppl = self.model.evaluate(arch, split='valid')\n",
    "                    except Exception as e:\n",
    "                        ppl = 1000000\n",
    "                    full_vals.append((arch, ppl))\n",
    "                full_vals = sorted(full_vals, key=lambda x:x[1])\n",
    "                logging.info('best arch: %s, best arch valid performance: %.3f' % (' '.join([str(i) for i in full_vals[0][0]]), full_vals[0][1]))\n",
    "                best_rounds.append(full_vals[0])\n",
    "            else:\n",
    "                best_rounds.append(sample_vals[0])\n",
    "        return best_rounds\n",
    "    \n",
    "    \n",
    "    def EA_arch_search(self,num_pop,num_ite,num_cross,num_mutation):\n",
    "\n",
    "        def get_init_pop(self,num_pop,n_nodes):\n",
    "            pop=np.empty((num_pop,8*n_nodes))\n",
    "            fitness=np.zeros((num_pop,))\n",
    "            for m in range(num_pop):         \n",
    "                k = sum(1 for i in range(self.model.model._steps) for n in range(2+i))\n",
    "                num_ops = len(genotypes.PRIMITIVES)\n",
    "                normal = []\n",
    "                reduction = []\n",
    "                for i in range(n_nodes):\n",
    "                    ops = np.random.choice(range(num_ops), 4)\n",
    "                    nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)\n",
    "                    nodes_in_reduce = np.random.choice(range(i+2), 2, replace=False)\n",
    "                    normal.extend([(nodes_in_normal[0], ops[0]), (nodes_in_normal[1], ops[1])])\n",
    "                    reduction.extend([(nodes_in_reduce[0], ops[2]), (nodes_in_reduce[1], ops[3])])\n",
    "                    pop[m,4*i]=nodes_in_normal[0]\n",
    "                    pop[m,4*i+1]=ops[0]\n",
    "                    pop[m,4*i+2]=nodes_in_normal[1]\n",
    "                    pop[m,4*i+3]=ops[1]\n",
    "                    pop[m,4*i+4*n_nodes]=nodes_in_reduce[0]\n",
    "                    pop[m,4*i+1+4*n_nodes]=ops[2]\n",
    "                    pop[m,4*i+2+4*n_nodes]=nodes_in_reduce[1]\n",
    "                    pop[m,4*i+3+4*n_nodes]=ops[3]                            \n",
    "                arch=(normal, reduction) \n",
    "                fitness[m,]=self.model.evaluate(arch)          \n",
    "            return pop,fitness\n",
    "\n",
    "\n",
    "        def corssover(self,pop,fitness,num_cross):\n",
    "            index=np.argsort(fitness)\n",
    "            pop_select=pop[index[0:num_cross],]\n",
    "\n",
    "\n",
    "            inde_cross=np.arange(num_cross)\n",
    "            np.random.shuffle(inde_cross)\n",
    "            pop_select=pop_select[inde_cross,]\n",
    "            pop_cross=np.empty((num_cross,pop.shape[1]))\n",
    "\n",
    "\n",
    "            for i in range(np.int(num_cross/2)):\n",
    "                cross1=pop_select[2*i,]\n",
    "                cross2=pop_select[2*i+1,]\n",
    "\n",
    "                cross_points=np.arange(4*self.model.model._steps)\n",
    "                np.random.shuffle(cross_points)\n",
    "                cross_points=cross_points[0:2]\n",
    "                cross_points=np.sort(cross_points)\n",
    "                p1=2*cross_points[0]\n",
    "                p2=2*cross_points[1]\n",
    "\n",
    "                cross1_=cross1\n",
    "                cross2_=cross2\n",
    "\n",
    "                cross1_[p1:p2]=cross2[p1:p2]\n",
    "                cross2_[p1:p2]=cross1[p1:p2]\n",
    "\n",
    "                pop_cross[2*i,]= cross1_       \n",
    "                pop_cross[2*i+1,]= cross2_   \n",
    "\n",
    "            return pop_cross\n",
    "\n",
    "\n",
    "        def mutation(self,pop,fitness,num_mutation):\n",
    "            index=np.argsort(fitness)\n",
    "            pop_select=pop[index[0:num_mutation],]\n",
    "            pop_mutation=np.empty((num_mutation,pop.shape[1]))\n",
    "            num_ops = len(genotypes.PRIMITIVES)\n",
    "\n",
    "\n",
    "            for i in range(num_mutation):\n",
    "                pop_mutation[i,]=pop_select[i,]\n",
    "\n",
    "                for j in range(pop.shape[1]):\n",
    "                    if j>((pop.shape[1])/2-1):\n",
    "                        q=j-(pop.shape[1])/2\n",
    "                    else:\n",
    "                        q=j\n",
    "                    m=q//4+2\n",
    "                    if np.random.rand()<0.2:#################genes with mutation probability 0.2\n",
    "                        if j%2==0:\n",
    "                            pop_mutation[i,j]=np.random.randint(m)\n",
    "                        else:\n",
    "                            pop_mutation[i,j]=np.random.randint(num_ops)            \n",
    "            return pop_mutation\n",
    "\n",
    "\n",
    "        def get_fitness(self,pop):\n",
    "            num_pop=pop.shape[0]\n",
    "            fitness=np.zeros((num_pop))\n",
    "            for m in range(num_pop):\n",
    "                indiv=pop[m,]\n",
    "                normal=[]\n",
    "                reduction=[]\n",
    "                for i in range(self.model.model._steps):\n",
    "                    s1=np.int(indiv[4*i,])\n",
    "                    s2=np.int(indiv[4*i+1,])\n",
    "                    s3=np.int(indiv[4*i+2,])\n",
    "                    s4=np.int(indiv[4*i+3,])\n",
    "                    s5=np.int(indiv[4*i+16,])\n",
    "                    s6=np.int(indiv[4*i+1+16,])\n",
    "                    s7=np.int(indiv[4*i+2+16,])\n",
    "                    s8=np.int(indiv[4*i+3+16,])\n",
    "                    normal.extend([(s1,s2), (s3,s4)])\n",
    "                    reduction.extend([(s5,s6), (s7,s8)]) \n",
    "                arch=(normal, reduction)\n",
    "                fitness[m,]=self.model.evaluate(arch)  \n",
    "\n",
    "            return fitness\n",
    "\n",
    "\n",
    "\n",
    "        k = sum(1 for i in range(self.model.model._steps) for n in range(2+i))\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "        n_nodes = self.model.model._steps    \n",
    "\n",
    "        pop,fitness=get_init_pop(self,num_pop,n_nodes)\n",
    "\n",
    "        for it in range(num_ite):\n",
    "            pop_cross=corssover(self,pop,fitness,num_cross)\n",
    "            fitness_cross=get_fitness(self,pop_cross)\n",
    "            pop_mutate=mutation(self,pop,fitness,num_mutation)\n",
    "            fitness_mutate=get_fitness(self,pop_mutate) \n",
    "            pop_comb=np.concatenate((pop,pop_cross,pop_mutate),axis=0)\n",
    "            fitness_comb=np.concatenate((fitness,fitness_cross,fitness_mutate),axis=0)\n",
    "            index=np.argsort(fitness_comb)\n",
    "            pop_comb=pop_comb[index,]\n",
    "            pop=pop_comb[0:num_pop,]\n",
    "            fitness=fitness_comb[0:num_pop,]\n",
    "\n",
    "        index=np.argsort(fitness)\n",
    "        indi_final=pop[index[0],]\n",
    "        \n",
    "        normal = []\n",
    "        reduction = []\n",
    "        for i in range(self.model.model._steps):\n",
    "\n",
    "            s1=np.int(indi_final[4*i,])\n",
    "            s2=np.int(indi_final[4*i+1,])\n",
    "            s3=np.int(indi_final[4*i+2,])\n",
    "            s4=np.int(indi_final[4*i+3,])\n",
    "            s5=np.int(indi_final[4*i+16,])\n",
    "            s6=np.int(indi_final[4*i+1+16,])\n",
    "            s7=np.int(indi_final[4*i+2+16,])\n",
    "            s8=np.int(indi_final[4*i+3+16,])\n",
    "            normal.extend([(s1,s2), (s3,s4)])\n",
    "            reduction.extend([(s5,s6), (s7,s8)]) \n",
    "        best_arch=(normal, reduction)\n",
    "\n",
    "        return best_arch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/05 05:46:14 PM Namespace(batch_size=64, benchmark='cnn', config='search', epochs=1, eval_only=0, grad_clip=5, init_channels=16, save_dir='/data/mzhang3/randomNAS_release-master/results', seed=100)\n",
      "06/05 05:46:17 PM param size = 1.930618MB\n",
      "06/05 05:46:17 PM param size: 1930618\n",
      "06/05 05:46:17 PM Args: {'data': '/data/mzhang3/randomNAS_own/data', 'epochs': 1, 'learning_rate': 0.025, 'batch_size': 64, 'learning_rate_min': 0.001, 'momentum': 0.9, 'weight_decay': 0.0003, 'init_channels': 16, 'layers': 8, 'drop_path_prob': 0.3, 'grad_clip': 5, 'train_portion': 0.5, 'seed': 100, 'log_interval': 50, 'save': '/data/mzhang3/randomNAS_release-master/results', 'gpu': 0, 'cuda': True, 'cutout': False, 'cutout_length': 16, 'report_freq': 50}\n",
      "06/05 05:46:17 PM Model total parameters: 1930618\n",
      "06/05 05:46:17 PM budget: 390\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-189426e3e460>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'budget: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0marchs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_eval_arch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;31m#archs = searcher.EA_arch_search(num_pop=50,num_ite=100,num_cross=30,num_mutation=20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a86f0b29487b>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_arch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#######################need to generate architecture based on novelty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a86f0b29487b>\u001b[0m in \u001b[0;36mget_arch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mnormal_arch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgene_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mreduce_arch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduce_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgene_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_normal_arch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormal_arch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore_reduce_arch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduce_arch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "sys.argv=['']; del sys\n",
    "parser = argparse.ArgumentParser(description='Args for SHA with weight sharing')\n",
    "parser.add_argument('--benchmark', dest='benchmark', type=str, default='cnn')\n",
    "parser.add_argument('--seed', dest='seed', type=int, default=100)\n",
    "parser.add_argument('--epochs', dest='epochs', type=int, default=1)\n",
    "parser.add_argument('--batch_size', dest='batch_size', type=int, default=64)\n",
    "parser.add_argument('--grad_clip', dest='grad_clip', type=float, default=5)\n",
    "parser.add_argument('--save_dir', dest='save_dir', type=str, default='/data/mzhang3/randomNAS_release-master/results')\n",
    "parser.add_argument('--eval_only', dest='eval_only', type=int, default=0)\n",
    "# PTB only argument. config=search uses proxy network for shared weights while\n",
    "# config=eval uses proxyless network for shared weights.\n",
    "parser.add_argument('--config', dest='config', type=str, default=\"search\")\n",
    "# CIFAR-10 only argument.  Use either 16 or 24 for the settings for random search\n",
    "# with weight-sharing used in our experiments.\n",
    "parser.add_argument('--init_channels', dest='init_channels', type=int, default=16)\n",
    "args = parser.parse_args()\n",
    "import sys \n",
    "    \n",
    "# Fill in with root output path\n",
    "root_dir = '/data/mzhang3/randomNAS_release-master/results'\n",
    "if args.save_dir is None:\n",
    "    save_dir = os.path.join(root_dir, '%s/random/trial%d' % (args.benchmark, args.seed))\n",
    "else:\n",
    "    save_dir = args.save_dir\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "if args.eval_only:\n",
    "    assert args.save_dir is not None\n",
    "\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(save_dir, 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "logging.info(args)\n",
    "\n",
    "if args.benchmark=='ptb':\n",
    "    data_size = 929589\n",
    "    time_steps = 35\n",
    "else:\n",
    "    data_size = 25000\n",
    "    time_steps = 1\n",
    "B = int(args.epochs * data_size / args.batch_size / time_steps)\n",
    "model = DartsWrapper(save_dir, args.seed, args.batch_size, args.grad_clip, args.epochs, init_channels=args.init_channels)\n",
    "\n",
    "searcher = Random_NAS(B, model, args.seed, save_dir)\n",
    "logging.info('budget: %d' % (searcher.B))\n",
    "if not args.eval_only:\n",
    "    searcher.run()\n",
    "    archs = searcher.get_eval_arch()\n",
    "    #archs = searcher.EA_arch_search(num_pop=50,num_ite=100,num_cross=30,num_mutation=20)\n",
    "\n",
    "else:\n",
    "    np.random.seed(args.seed+1)\n",
    "    archs = searcher.get_eval_arch(2)\n",
    "logging.info(archs)\n",
    "arch = ' '.join([str(a) for a in archs[0][0]])\n",
    "with open('/tmp/arch','w') as f:\n",
    "    f.write(arch)\n",
    "\n",
    "       \n",
    "print(arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([(0, 1), (0, 7), (2, 3), (0, 0), (1, 0), (2, 0), (1, 4), (2, 6)], [(1, 3), (1, 7), (2, 0), (2, 6), (2, 6), (3, 6), (0, 0), (2, 4)])\n"
     ]
    }
   ],
   "source": [
    "print(archs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "([(1, 5), (1, 6), (1, 6), (1, 4), (1, 5), (0, 2), (0, 3), (3, 0)], [(0, 6), (0, 0), (0, 6), (1, 2), (2, 7), (3, 7), (2, 0), (3, 4)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
