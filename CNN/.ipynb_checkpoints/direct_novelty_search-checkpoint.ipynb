{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######code for directly calculate distance between stored architectures(last 1000)\n",
    "\n",
    "\n",
    "\n",
    "import sys\n",
    "import genotypes\n",
    "from model_search import Network\n",
    "import utils\n",
    "\n",
    "import time\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import logging\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "sys.path.append('/data/mzhang3/randomNAS_release-master')\n",
    "import shutil\n",
    "import inspect\n",
    "import pickle\n",
    "import argparse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "class DartsWrapper:\n",
    "    def __init__(self, save_path, seed, batch_size, grad_clip, epochs, resume_iter=None, init_channels=16):\n",
    "        args = {}\n",
    "        args['data'] = '/data/mzhang3/randomNAS_own/data'\n",
    "        args['epochs'] = epochs\n",
    "        args['learning_rate'] = 0.025\n",
    "        args['batch_size'] = batch_size\n",
    "        args['learning_rate_min'] = 0.001\n",
    "        args['momentum'] = 0.9\n",
    "        args['weight_decay'] = 3e-4\n",
    "        args['init_channels'] = init_channels\n",
    "        args['layers'] = 8\n",
    "        args['drop_path_prob'] = 0.3\n",
    "        args['grad_clip'] = grad_clip\n",
    "        args['train_portion'] = 0.5\n",
    "        args['seed'] = seed\n",
    "        args['log_interval'] = 50\n",
    "        args['save'] = save_path\n",
    "        args['gpu'] = 0\n",
    "        args['cuda'] = True\n",
    "        args['cutout'] = False\n",
    "        args['cutout_length'] = 16\n",
    "        args['report_freq'] = 50\n",
    "        args = AttrDict(args)\n",
    "        self.args = args\n",
    "        self.seed = seed\n",
    "\n",
    "        np.random.seed(args.seed)\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "        cudnn.benchmark = False\n",
    "        cudnn.enabled=True\n",
    "        cudnn.deterministic=True\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "        train_transform, valid_transform = utils._data_transforms_cifar10(args)\n",
    "        train_data = dset.CIFAR10(root=args.data, train=True, download=False, transform=train_transform)\n",
    "\n",
    "        num_train = len(train_data)\n",
    "        indices = list(range(num_train))\n",
    "        split = int(np.floor(args.train_portion * num_train))\n",
    "\n",
    "        self.train_queue = torch.utils.data.DataLoader(\n",
    "          train_data, batch_size=args.batch_size,\n",
    "          sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "          pin_memory=True, num_workers=0, worker_init_fn=np.random.seed(args.seed))\n",
    "\n",
    "        self.valid_queue = torch.utils.data.DataLoader(\n",
    "          train_data, batch_size=args.batch_size,\n",
    "          sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "          pin_memory=True, num_workers=0, worker_init_fn=np.random.seed(args.seed))\n",
    "\n",
    "        self.train_iter = iter(self.train_queue)\n",
    "        self.valid_iter = iter(self.valid_queue)\n",
    "\n",
    "        self.steps = 0\n",
    "        self.epochs = 0\n",
    "        self.total_loss = 0\n",
    "        self.start_time = time.time()\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        criterion = criterion.cuda()\n",
    "        self.criterion = criterion\n",
    "\n",
    "        model = Network(args.init_channels, 10, args.layers, self.criterion)\n",
    "\n",
    "        model = model.cuda()\n",
    "        self.model = model\n",
    "\n",
    "        try:\n",
    "            self.load()\n",
    "            logging.info('loaded previously saved weights')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "        logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\n",
    "\n",
    "        optimizer = torch.optim.SGD(\n",
    "          self.model.parameters(),\n",
    "          args.learning_rate,\n",
    "          momentum=args.momentum,\n",
    "          weight_decay=args.weight_decay)\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "          optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
    "\n",
    "        if resume_iter is not None:\n",
    "            self.steps = resume_iter\n",
    "            self.epochs = int(resume_iter / len(self.train_queue))\n",
    "            logging.info(\"Resuming from epoch %d\" % self.epochs)\n",
    "            self.objs = utils.AvgrageMeter()\n",
    "            self.top1 = utils.AvgrageMeter()\n",
    "            self.top5 = utils.AvgrageMeter()\n",
    "            for i in range(self.epochs):\n",
    "                self.scheduler.step()\n",
    "\n",
    "        size = 0\n",
    "        for p in model.parameters():\n",
    "            size += p.nelement()\n",
    "        logging.info('param size: {}'.format(size))\n",
    "\n",
    "        total_params = sum(x.data.nelement() for x in model.parameters())\n",
    "        logging.info('Args: {}'.format(args))\n",
    "        logging.info('Model total parameters: {}'.format(total_params))\n",
    "\n",
    "    def train_batch(self, arch):\n",
    "        args = self.args\n",
    "        if self.steps % len(self.train_queue) == 0:\n",
    "            \n",
    "            self.scheduler.step()\n",
    "            self.objs = utils.AvgrageMeter()\n",
    "            self.top1 = utils.AvgrageMeter()\n",
    "            self.top5 = utils.AvgrageMeter()\n",
    "        lr = self.scheduler.get_lr()[0]\n",
    "\n",
    "        weights = self.get_weights_from_arch(arch)\n",
    "        self.set_model_weights(weights)\n",
    "\n",
    "        step = self.steps % len(self.train_queue)\n",
    "        input, target = next(self.train_iter)\n",
    "\n",
    "        self.model.train()\n",
    "        n = input.size(0)\n",
    "\n",
    "        input = Variable(input, requires_grad=False).cuda()\n",
    "        target = Variable(target, requires_grad=False).cuda(async=True)\n",
    "\n",
    "      # get a random minibatch from the search queue with replacement\n",
    "        self.optimizer.zero_grad()\n",
    "        logits = self.model(input)\n",
    "        loss = self.criterion(logits, target)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(self.model.parameters(), args.grad_clip)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        self.objs.update(loss.data, n)\n",
    "        self.top1.update(prec1.data, n)\n",
    "        self.top5.update(prec5.data, n)\n",
    "\n",
    "        if step % args.report_freq == 0:\n",
    "            logging.info('train %03d %e %f %f', step, self.objs.avg, self.top1.avg, self.top5.avg)\n",
    "\n",
    "        self.steps += 1\n",
    "        if self.steps % len(self.train_queue) == 0:\n",
    "            self.epochs += 1\n",
    "            self.train_iter = iter(self.train_queue)\n",
    "            valid_err = self.evaluate(arch)\n",
    "            logging.info('epoch %d  |  train_acc %f  |  valid_acc %f' % (self.epochs, self.top1.avg, 1-valid_err))\n",
    "            self.save()\n",
    "\n",
    "    def evaluate(self, arch, split=None):\n",
    "      # Return error since we want to minimize obj val\n",
    "        logging.info(arch)\n",
    "        objs = utils.AvgrageMeter()\n",
    "        top1 = utils.AvgrageMeter()\n",
    "        top5 = utils.AvgrageMeter()\n",
    "\n",
    "        weights = self.get_weights_from_arch(arch)\n",
    "        self.set_model_weights(weights)\n",
    "\n",
    "        self.model.eval()\n",
    "\n",
    "        if split is None:\n",
    "            n_batches = 10\n",
    "        else:\n",
    "            n_batches = len(self.valid_queue)\n",
    "\n",
    "        for step in range(n_batches):\n",
    "            try:\n",
    "                input, target = next(self.valid_iter)\n",
    "            except Exception as e:\n",
    "                logging.info('looping back over valid set')\n",
    "                self.valid_iter = iter(self.valid_queue)\n",
    "                input, target = next(self.valid_iter)\n",
    "            input = Variable(input).cuda()\n",
    "            target = Variable(target).cuda(async=True)\n",
    "\n",
    "            logits = self.model(input)\n",
    "            loss = self.criterion(logits, target)\n",
    "\n",
    "            prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "            n = input.size(0)\n",
    "            objs.update(loss.data, n)\n",
    "            top1.update(prec1.data, n)\n",
    "            top5.update(prec5.data, n)\n",
    "\n",
    "            if step % self.args.report_freq == 0:\n",
    "                logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "\n",
    "        return 1-top1.avg\n",
    "\n",
    "    def save(self):\n",
    "        utils.save(self.model, os.path.join(self.args.save, 'weights.pt'))\n",
    "\n",
    "    def load(self):\n",
    "        utils.load(self.model, os.path.join(self.args.save, 'weights.pt'))\n",
    "\n",
    "    def get_weights_from_arch(self, arch):\n",
    "        k = sum(1 for i in range(self.model._steps) for n in range(2+i))\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "        n_nodes = self.model._steps\n",
    "\n",
    "        alphas_normal = Variable(torch.zeros(k, num_ops).cuda(), requires_grad=False)\n",
    "        alphas_reduce = Variable(torch.zeros(k, num_ops).cuda(), requires_grad=False)\n",
    "\n",
    "        offset = 0\n",
    "        for i in range(n_nodes):\n",
    "            normal1 = arch[0][2*i]\n",
    "            normal2 = arch[0][2*i+1]\n",
    "            reduce1 = arch[1][2*i]\n",
    "            reduce2 = arch[1][2*i+1]\n",
    "            alphas_normal[offset+normal1[0], normal1[1]] = 1\n",
    "            alphas_normal[offset+normal2[0], normal2[1]] = 1\n",
    "            alphas_reduce[offset+reduce1[0], reduce1[1]] = 1\n",
    "            alphas_reduce[offset+reduce2[0], reduce2[1]] = 1\n",
    "            offset += (i+2)\n",
    "\n",
    "        arch_parameters = [\n",
    "          alphas_normal,\n",
    "          alphas_reduce,\n",
    "        ]\n",
    "        return arch_parameters\n",
    "\n",
    "    def set_model_weights(self, weights):\n",
    "        self.model.alphas_normal = weights[0]\n",
    "        self.model.alphas_reduce = weights[1]\n",
    "        self.model._arch_parameters = [self.model.alphas_normal, self.model.alphas_reduce]\n",
    "        \n",
    "\n",
    "    def novelty_fitness(self,arch,store_arch,k):\n",
    "\n",
    "        def mapping(sample_arch):\n",
    "            arch_map = np.zeros((14,7))\n",
    "            for i in range(8):\n",
    "                ind_node=i//2\n",
    "                k = sum(1 for i in range(ind_node) for n in range(2+i))\n",
    "                ind_nomal_op=np.int(sample_arch[2*i+1])\n",
    "                ind_nomal_con=np.int(k+sample_arch[2*i])\n",
    "                arch_map[ind_nomal_con,ind_nomal_op]=arch_map[ind_nomal_con,ind_nomal_op]+1\n",
    "            return arch_map\n",
    "\n",
    "        def cal_vec_dis(vec1,vec2):\n",
    "            if  np.linalg.norm(vec1-vec2)==0:\n",
    "                dis=0\n",
    "            else:\n",
    "                dis=1\n",
    "            return dis\n",
    "\n",
    "        def dis_arch(arch1,arch2):\n",
    "            mat1=mapping(arch1)\n",
    "            mat2=mapping(arch2)\n",
    "            dis=0\n",
    "            for i in range(mat1.shape[0]):\n",
    "                dis=dis+cal_vec_dis(mat1[i,],mat2[i,])\n",
    "            dis=dis/mat1.shape[0]\n",
    "            return dis\n",
    "        \n",
    "        store_arch=store_arch[:-1000,]\n",
    "        \n",
    "        n_connec=2*self.model._steps              \n",
    "        arch=np.array(arch)        \n",
    "        arch=np.reshape(arch,(1,n_connec*2))\n",
    "\n",
    "        dis=np.zeros((store_arch.shape[0],))\n",
    "        for i in range(store_arch.shape[0]):\n",
    "            dis[i]=dis_arch(arch[0,],store_arch[i,])\n",
    "        sort_dis=np.sort(dis)\n",
    "        novelty_dis=np.mean(sort_dis[0:k])\n",
    "        \n",
    "        return novelty_dis      \n",
    "        \n",
    "        \n",
    "\n",
    "    def sample_arch_eval(self):\n",
    "        k = sum(1 for i in range(self.model._steps) for n in range(2+i))\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "        n_nodes = self.model._steps\n",
    "\n",
    "        normal = []\n",
    "        reduction = []\n",
    "        for i in range(n_nodes):\n",
    "            ops = np.random.choice(range(num_ops), 4)\n",
    "            nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)\n",
    "            nodes_in_reduce = np.random.choice(range(i+2), 2, replace=False)\n",
    "            normal.extend([(nodes_in_normal[0], ops[0]), (nodes_in_normal[1], ops[1])])\n",
    "            reduction.extend([(nodes_in_reduce[0], ops[2]), (nodes_in_reduce[1], ops[3])])\n",
    "        return (normal, reduction)    \n",
    "        \n",
    "    def sample_arch(self,store_normal_arch,store_reduce_arch):\n",
    "       \n",
    "        if sum(sum(store_normal_arch))>1:\n",
    "                                            \n",
    "            gene_num =  100\n",
    "            k = sum(1 for i in range(self.model._steps) for n in range(2+i))\n",
    "            num_ops = len(genotypes.PRIMITIVES)\n",
    "            n_nodes = self.model._steps\n",
    "            n_connec=2*n_nodes\n",
    "            normal_generate=np.empty((8*gene_num,2))\n",
    "            reduce_generate=np.empty((8*gene_num,2))\n",
    "            normal_novelty_fitness=np.zeros((gene_num))\n",
    "            reduce_novelty_fitness=np.zeros((gene_num))\n",
    "            for i in range(gene_num):                \n",
    "                \n",
    "                for mm in range(n_nodes):\n",
    "                \n",
    "                    ops = np.random.choice(range(num_ops), 4)\n",
    "                    nodes_in_normal = sorted(np.random.choice(range(mm+2), 2, replace=False))\n",
    "                    nodes_in_reduce = sorted(np.random.choice(range(mm+2), 2, replace=False))\n",
    "                    normal_generate[i*(2*n_nodes)+mm*2,0]=nodes_in_normal[0]\n",
    "                    normal_generate[i*(2*n_nodes)+mm*2,1]=ops[0]\n",
    "                    normal_generate[i*(2*n_nodes)+mm*2+1,0]=nodes_in_normal[1]\n",
    "                    normal_generate[i*(2*n_nodes)+mm*2+1,1]=ops[1]\n",
    "                \n",
    "                    reduce_generate[i*(2*n_nodes)+mm*2,0]=nodes_in_reduce[0]\n",
    "                    reduce_generate[i*(2*n_nodes)+mm*2,1]=ops[2]\n",
    "                    reduce_generate[i*(2*n_nodes)+mm*2+1,0]=nodes_in_reduce[1]\n",
    "                    reduce_generate[i*(2*n_nodes)+mm*2+1,1]=ops[3] \n",
    "                    \n",
    "                normal_novelty_fitness[i]=self.novelty_fitness(normal_generate[i*8:i*8+8,],store_normal_arch,100)\n",
    "                reduce_novelty_fitness[i]=self.novelty_fitness(reduce_generate[i*8:i*8+8,],store_reduce_arch,100)    \n",
    "                                      \n",
    "            \n",
    "            sort_normal=np.argsort(normal_novelty_fitness)\n",
    "            sort_reduce=np.argsort(reduce_novelty_fitness)\n",
    "            ind_normal=sort_normal[0]\n",
    "            ind_reduce=sort_reduce[0]\n",
    "            \n",
    "           \n",
    "            selec_normal=normal_generate[ind_normal*n_connec:ind_normal*n_connec+n_connec,]\n",
    "            selec_reduce=reduce_generate[ind_reduce*n_connec:ind_reduce*n_connec+n_connec,]\n",
    "\n",
    "            \n",
    "            normal = []\n",
    "            reduction = []\n",
    "            for i in range(n_nodes):\n",
    "                s1=np.int(selec_normal[2*i,0])\n",
    "                s2=np.int(selec_normal[2*i,1])\n",
    "                s3=np.int(selec_normal[2*i+1,0])\n",
    "                s4=np.int(selec_normal[2*i+1,1])\n",
    "                s5=np.int(selec_reduce[2*i,0])\n",
    "                s6=np.int(selec_reduce[2*i,1])\n",
    "                s7=np.int(selec_reduce[2*i+1,0])\n",
    "                s8=np.int(selec_reduce[2*i+1,1])\n",
    "                normal.extend([(s1,s2), (s3,s4)])\n",
    "                reduction.extend([(s5,s6), (s7,s8)])                                          \n",
    "        else:     \n",
    "            k = sum(1 for i in range(self.model._steps) for n in range(2+i))\n",
    "            num_ops = len(genotypes.PRIMITIVES)\n",
    "            n_nodes = self.model._steps\n",
    "\n",
    "            normal = []\n",
    "            reduction = []\n",
    "            for i in range(n_nodes):\n",
    "                ops = np.random.choice(range(num_ops), 4)\n",
    "                nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)                \n",
    "                nodes_in_reduce = np.random.choice(range(i+2), 2, replace=False)                \n",
    "                normal.extend([(nodes_in_normal[0], ops[0]), (nodes_in_normal[1], ops[1])])\n",
    "                \n",
    "                reduction.extend([(nodes_in_reduce[0], ops[2]), (nodes_in_reduce[1], ops[3])])\n",
    "                \n",
    "            normal=np.int_(normal)\n",
    "            reduction=np.int_(reduction)\n",
    "######the operations from two previous node are different\n",
    "        return (normal, reduction)\n",
    "\n",
    "\n",
    "    def perturb_arch(self, arch):\n",
    "        new_arch = copy.deepcopy(arch)\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "\n",
    "        cell_ind = np.random.choice(2)\n",
    "        step_ind = np.random.choice(self.model._steps)\n",
    "        nodes_in = np.random.choice(step_ind+2, 2, replace=False)\n",
    "        ops = np.random.choice(range(num_ops), 2)\n",
    "\n",
    "        new_arch[cell_ind][2*step_ind] = (nodes_in[0], ops[0])\n",
    "        new_arch[cell_ind][2*step_ind+1] = (nodes_in[1], ops[1])\n",
    "        return new_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rung:\n",
    "    def __init__(self, rung, nodes):\n",
    "        self.parents = set()\n",
    "        self.children = set()\n",
    "        self.rung = rung\n",
    "        for node in nodes:\n",
    "            n = nodes[node]\n",
    "            if n.rung == self.rung:\n",
    "                self.parents.add(n.parent)\n",
    "                self.children.add(n.node_id)\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, parent, arch, node_id, rung):\n",
    "        self.parent = parent\n",
    "        self.arch = arch\n",
    "        self.node_id = node_id\n",
    "        self.rung = rung\n",
    "     #  self.objective_val = self.model.evaluate(arch)  \n",
    "    def to_dict(self):\n",
    "        out = {'parent':self.parent, 'arch': self.arch, 'node_id': self.node_id, 'rung': self.rung}\n",
    "        if hasattr(self, 'objective_val'):\n",
    "            out['objective_val'] = self.objective_val\n",
    "        return out\n",
    "\n",
    "class Random_NAS:\n",
    "    def __init__(self, B, model, seed, save_dir):\n",
    "        self.save_dir = save_dir\n",
    "\n",
    "        self.B = B\n",
    "        self.model = model\n",
    "        self.seed = seed\n",
    "        self.iters = 0\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "        k = sum(1 for i in range(self.model.model._steps) for n in range(2+i))\n",
    "\n",
    "        self.arms = {}\n",
    "        size_arch=self.model.model._steps*4\n",
    "               \n",
    "        self.store_normal_arch=np.empty((1,size_arch))\n",
    "        self.store_reduce_arch=np.zeros((1,size_arch))\n",
    "        \n",
    "        \n",
    "        self.node_id = 0\n",
    "\n",
    "    def print_summary(self):\n",
    "        logging.info(self.parents)\n",
    "        objective_vals = [(n,self.arms[n].objective_val) for n in self.arms if hasattr(self.arms[n],'objective_val')]\n",
    "        objective_vals = sorted(objective_vals,key=lambda x:x[1])\n",
    "        best_arm = self.arms[objective_vals[0][0]]\n",
    "        val_ppl = self.model.evaluate(best_arm.arch, split='valid')\n",
    "        logging.info(objective_vals)\n",
    "        logging.info('best valid ppl: %.2f' % val_ppl)\n",
    "\n",
    "\n",
    "    def get_arch(self):####need to generate architecture based on novelty    \n",
    "        arch = self.model.sample_arch(self.store_normal_arch,self.store_reduce_arch)\n",
    "        normal_arch=np.array(arch[0])\n",
    "        reduce_arch=np.array(arch[1])\n",
    "        \n",
    "        gene_len=self.model.model._steps*4\n",
    "        num_con=self.model.model._steps*2\n",
    "        \n",
    "        normal_arch=np.reshape(normal_arch,(1,gene_len))\n",
    "        reduce_arch=np.reshape(reduce_arch,(1,gene_len))\n",
    "        store_normal_arch=np.concatenate((self.store_normal_arch,normal_arch),axis=0)\n",
    "        store_reduce_arch=np.concatenate((self.store_reduce_arch,reduce_arch),axis=0) \n",
    "        \n",
    "        self.arms[self.node_id] = Node(self.node_id, arch, self.node_id, 0)\n",
    "\n",
    "        self.node_id += 1\n",
    "        return arch\n",
    "\n",
    "    def save(self):\n",
    "        to_save = {a: self.arms[a].to_dict() for a in self.arms}\n",
    "        # Only replace file if save successful so don't lose results of last pickle save\n",
    "        with open(os.path.join(self.save_dir,'results_tmp.pkl'),'wb') as f:\n",
    "            pickle.dump(to_save, f)\n",
    "        shutil.copyfile(os.path.join(self.save_dir, 'results_tmp.pkl'), os.path.join(self.save_dir, 'results.pkl'))\n",
    "\n",
    "        self.model.save()\n",
    "\n",
    "    def run(self):\n",
    "        while self.iters < self.B:\n",
    "            arch = self.get_arch()#######################need to generate architecture based on novelty\n",
    "            self.model.train_batch(arch)\n",
    "            self.iters += 1\n",
    "\n",
    "            if self.iters % 500 == 0:\n",
    "                self.save()\n",
    "        self.save()\n",
    "\n",
    "    def get_eval_arch(self, rounds=None):\n",
    "        #n_rounds = int(self.B / 7 / 1000)\n",
    "        if rounds is None:\n",
    "            n_rounds = max(1,int(self.B/10000))\n",
    "        else:\n",
    "            n_rounds = rounds\n",
    "        best_rounds = []\n",
    "        for r in range(n_rounds):\n",
    "            sample_vals = []\n",
    "            for _ in range(1000):\n",
    "                arch = self.model.sample_arch_eval()\n",
    "                try:\n",
    "                    ppl = self.model.evaluate(arch)               \n",
    "                except Exception as e:\n",
    "                    ppl = 1000000\n",
    "                logging.info(arch)\n",
    "                logging.info('objective_val: %.3f' % ppl)\n",
    "                sample_vals.append((arch, ppl))\n",
    "            sample_vals = sorted(sample_vals, key=lambda x:x[1])\n",
    "\n",
    "            full_vals = []\n",
    "            if 'split' in inspect.getargspec(self.model.evaluate).args:\n",
    "                for i in range(10):\n",
    "                    arch = sample_vals[i][0]\n",
    "                    try:\n",
    "                        ppl = self.model.evaluate(arch, split='valid')\n",
    "                    except Exception as e:\n",
    "                        ppl = 1000000\n",
    "                    full_vals.append((arch, ppl))\n",
    "                full_vals = sorted(full_vals, key=lambda x:x[1])\n",
    "                logging.info('best arch: %s, best arch valid performance: %.3f' % (' '.join([str(i) for i in full_vals[0][0]]), full_vals[0][1]))\n",
    "                best_rounds.append(full_vals[0])\n",
    "            else:\n",
    "                best_rounds.append(sample_vals[0])\n",
    "        return best_rounds\n",
    "    \n",
    "    \n",
    "    \n",
    "    def EA_arch_search(self,num_pop,num_ite,num_cross,num_mutation):\n",
    "\n",
    "        def get_init_pop(self,num_pop,n_nodes):\n",
    "            pop=np.empty((num_pop,8*n_nodes))\n",
    "            fitness=np.zeros((num_pop,))\n",
    "            for m in range(num_pop):         \n",
    "                k = sum(1 for i in range(self.model.model._steps) for n in range(2+i))\n",
    "                num_ops = len(genotypes.PRIMITIVES)\n",
    "                normal = []\n",
    "                reduction = []\n",
    "                for i in range(n_nodes):\n",
    "                    ops = np.random.choice(range(num_ops), 4)\n",
    "                    nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)\n",
    "                    nodes_in_reduce = np.random.choice(range(i+2), 2, replace=False)\n",
    "                    normal.extend([(nodes_in_normal[0], ops[0]), (nodes_in_normal[1], ops[1])])\n",
    "                    reduction.extend([(nodes_in_reduce[0], ops[2]), (nodes_in_reduce[1], ops[3])])\n",
    "                    pop[m,4*i]=nodes_in_normal[0]\n",
    "                    pop[m,4*i+1]=ops[0]\n",
    "                    pop[m,4*i+2]=nodes_in_normal[1]\n",
    "                    pop[m,4*i+3]=ops[1]\n",
    "                    pop[m,4*i+4*n_nodes]=nodes_in_reduce[0]\n",
    "                    pop[m,4*i+1+4*n_nodes]=ops[2]\n",
    "                    pop[m,4*i+2+4*n_nodes]=nodes_in_reduce[1]\n",
    "                    pop[m,4*i+3+4*n_nodes]=ops[3]                            \n",
    "                arch=(normal, reduction) \n",
    "                fitness[m,]=self.model.evaluate(arch)          \n",
    "            return pop,fitness\n",
    "\n",
    "\n",
    "        def corssover(self,pop,fitness,num_cross):\n",
    "            index=np.argsort(fitness)\n",
    "            pop_select=pop[index[0:num_cross],]\n",
    "\n",
    "\n",
    "            inde_cross=np.arange(num_cross)\n",
    "            np.random.shuffle(inde_cross)\n",
    "            pop_select=pop_select[inde_cross,]\n",
    "            pop_cross=np.empty((num_cross,pop.shape[1]))\n",
    "\n",
    "\n",
    "            for i in range(np.int(num_cross/2)):\n",
    "                cross1=pop_select[2*i,]\n",
    "                cross2=pop_select[2*i+1,]\n",
    "\n",
    "                cross_points=np.arange(4*self.model.model._steps)\n",
    "                np.random.shuffle(cross_points)\n",
    "                cross_points=cross_points[0:2]\n",
    "                cross_points=np.sort(cross_points)\n",
    "                p1=2*cross_points[0]\n",
    "                p2=2*cross_points[1]\n",
    "\n",
    "                cross1_=cross1\n",
    "                cross2_=cross2\n",
    "\n",
    "                cross1_[p1:p2]=cross2[p1:p2]\n",
    "                cross2_[p1:p2]=cross1[p1:p2]\n",
    "\n",
    "                pop_cross[2*i,]= cross1_       \n",
    "                pop_cross[2*i+1,]= cross2_   \n",
    "\n",
    "            return pop_cross\n",
    "\n",
    "\n",
    "        def mutation(self,pop,fitness,num_mutation):\n",
    "            index=np.argsort(fitness)\n",
    "            pop_select=pop[index[0:num_mutation],]\n",
    "            pop_mutation=np.empty((num_mutation,pop.shape[1]))\n",
    "            num_ops = len(genotypes.PRIMITIVES)\n",
    "\n",
    "\n",
    "            for i in range(num_mutation):\n",
    "                pop_mutation[i,]=pop_select[i,]\n",
    "\n",
    "                for j in range(pop.shape[1]):\n",
    "                    if j>((pop.shape[1])/2-1):\n",
    "                        q=j-(pop.shape[1])/2\n",
    "                    else:\n",
    "                        q=j\n",
    "                    m=q//4+2\n",
    "                    if np.random.rand()<0.2:#################genes with mutation probability 0.2\n",
    "                        if j%2==0:\n",
    "                            pop_mutation[i,j]=np.random.randint(m)\n",
    "                        else:\n",
    "                            pop_mutation[i,j]=np.random.randint(num_ops)            \n",
    "            return pop_mutation\n",
    "\n",
    "\n",
    "        def get_fitness(self,pop):\n",
    "            num_pop=pop.shape[0]\n",
    "            fitness=np.zeros((num_pop))\n",
    "            for m in range(num_pop):\n",
    "                indiv=pop[m,]\n",
    "                normal=[]\n",
    "                reduction=[]\n",
    "                for i in range(self.model.model._steps):\n",
    "                    s1=np.int(indiv[4*i,])\n",
    "                    s2=np.int(indiv[4*i+1,])\n",
    "                    s3=np.int(indiv[4*i+2,])\n",
    "                    s4=np.int(indiv[4*i+3,])\n",
    "                    s5=np.int(indiv[4*i+16,])\n",
    "                    s6=np.int(indiv[4*i+1+16,])\n",
    "                    s7=np.int(indiv[4*i+2+16,])\n",
    "                    s8=np.int(indiv[4*i+3+16,])\n",
    "                    normal.extend([(s1,s2), (s3,s4)])\n",
    "                    reduction.extend([(s5,s6), (s7,s8)]) \n",
    "                arch=(normal, reduction)\n",
    "                fitness[m,]=self.model.evaluate(arch)  \n",
    "\n",
    "            return fitness\n",
    "\n",
    "\n",
    "\n",
    "        k = sum(1 for i in range(self.model.model._steps) for n in range(2+i))\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "        n_nodes = self.model.model._steps    \n",
    "\n",
    "        pop,fitness=get_init_pop(self,num_pop,n_nodes)\n",
    "\n",
    "        for it in range(num_ite):\n",
    "            pop_cross=corssover(self,pop,fitness,num_cross)\n",
    "            fitness_cross=get_fitness(self,pop_cross)\n",
    "            pop_mutate=mutation(self,pop,fitness,num_mutation)\n",
    "            fitness_mutate=get_fitness(self,pop_mutate) \n",
    "            pop_comb=np.concatenate((pop,pop_cross,pop_mutate),axis=0)\n",
    "            fitness_comb=np.concatenate((fitness,fitness_cross,fitness_mutate),axis=0)\n",
    "            index=np.argsort(fitness_comb)\n",
    "            pop_comb=pop_comb[index,]\n",
    "            pop=pop_comb[0:num_pop,]\n",
    "            fitness=fitness_comb[0:num_pop,]\n",
    "\n",
    "        index=np.argsort(fitness)\n",
    "        indi_final=pop[index[0],]\n",
    "        \n",
    "        normal = []\n",
    "        reduction = []\n",
    "        for i in range(self.model.model._steps):\n",
    "\n",
    "            s1=np.int(indi_final[4*i,])\n",
    "            s2=np.int(indi_final[4*i+1,])\n",
    "            s3=np.int(indi_final[4*i+2,])\n",
    "            s4=np.int(indi_final[4*i+3,])\n",
    "            s5=np.int(indi_final[4*i+16,])\n",
    "            s6=np.int(indi_final[4*i+1+16,])\n",
    "            s7=np.int(indi_final[4*i+2+16,])\n",
    "            s8=np.int(indi_final[4*i+3+16,])\n",
    "            normal.extend([(s1,s2), (s3,s4)])\n",
    "            reduction.extend([(s5,s6), (s7,s8)]) \n",
    "        best_arch=(normal, reduction)\n",
    "\n",
    "        return best_arch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/22 08:53:17 PM Namespace(batch_size=16, benchmark='cnn', config='search', epochs=2, eval_only=0, grad_clip=5, init_channels=16, save_dir='/data/mzhang3/randomNAS_release-master/results', seed=100)\n",
      "04/22 08:53:21 PM loaded previously saved weights\n",
      "04/22 08:53:21 PM param size = 1.930618MB\n",
      "04/22 08:53:21 PM param size: 1930618\n",
      "04/22 08:53:21 PM Args: {'data': '/data/mzhang3/randomNAS_own/data', 'epochs': 2, 'learning_rate': 0.025, 'batch_size': 16, 'learning_rate_min': 0.001, 'momentum': 0.9, 'weight_decay': 0.0003, 'init_channels': 16, 'layers': 8, 'drop_path_prob': 0.3, 'grad_clip': 5, 'train_portion': 0.5, 'seed': 100, 'log_interval': 50, 'save': '/data/mzhang3/randomNAS_release-master/results', 'gpu': 0, 'cuda': True, 'cutout': False, 'cutout_length': 16, 'report_freq': 50}\n",
      "04/22 08:53:21 PM Model total parameters: 1930618\n",
      "04/22 08:53:21 PM budget: 3125\n",
      "04/22 08:53:22 PM train 000 2.824531e-01 87.500000 100.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "__main__:171: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/22 08:53:46 PM train 050 2.933387e-01 89.583336 99.509804\n",
      "04/22 08:54:10 PM train 100 3.048980e-01 89.418312 99.690590\n",
      "04/22 08:54:34 PM train 150 3.510816e-01 88.286423 99.668877\n",
      "04/22 08:54:57 PM train 200 3.557887e-01 88.152985 99.595772\n",
      "04/22 08:55:21 PM train 250 3.661074e-01 87.724106 99.601593\n",
      "04/22 08:55:45 PM train 300 3.716844e-01 87.500000 99.522423\n",
      "04/22 08:56:09 PM train 350 3.627222e-01 87.731483 99.554840\n",
      "04/22 08:56:33 PM train 400 3.714826e-01 87.312973 99.579178\n",
      "04/22 08:56:57 PM train 450 3.688147e-01 87.486137 99.501106\n",
      "04/22 08:57:21 PM train 500 3.727280e-01 87.300400 99.500999\n",
      "04/22 08:57:45 PM train 550 3.738694e-01 87.227768 99.523590\n",
      "04/22 08:58:09 PM train 600 3.841548e-01 86.813644 99.521629\n",
      "04/22 08:58:33 PM train 650 3.870896e-01 86.703148 99.519966\n",
      "04/22 08:58:59 PM train 700 3.835437e-01 86.866982 99.527466\n",
      "04/22 08:59:23 PM train 750 3.862245e-01 86.859184 99.517303\n",
      "04/22 08:59:47 PM train 800 3.854581e-01 86.867973 99.500626\n",
      "04/22 09:00:11 PM train 850 3.866521e-01 86.875732 99.463867\n",
      "04/22 09:00:35 PM train 900 3.875238e-01 86.847946 99.465874\n",
      "04/22 09:01:00 PM train 950 3.895573e-01 86.790215 99.454514\n",
      "04/22 09:01:24 PM train 1000 3.907005e-01 86.769478 99.456787\n",
      "04/22 09:01:48 PM train 1050 3.891000e-01 86.798286 99.446953\n",
      "04/22 09:02:14 PM train 1100 3.899919e-01 86.722298 99.438011\n",
      "04/22 09:02:39 PM train 1150 3.903719e-01 86.701782 99.440704\n",
      "04/22 09:03:04 PM train 1200 3.900774e-01 86.740219 99.422356\n",
      "04/22 09:03:29 PM train 1250 3.906296e-01 86.715630 99.425461\n",
      "04/22 09:03:54 PM train 1300 3.907293e-01 86.721748 99.423515\n",
      "04/22 09:04:18 PM train 1350 3.919173e-01 86.727425 99.403214\n",
      "04/22 09:04:45 PM train 1400 3.911313e-01 86.754997 99.411133\n",
      "04/22 09:05:13 PM train 1450 3.919394e-01 86.690216 99.414200\n",
      "04/22 09:05:40 PM train 1500 3.921027e-01 86.663063 99.421219\n",
      "04/22 09:06:08 PM train 1550 3.912252e-01 86.702126 99.419724\n",
      "04/22 09:06:15 PM (array([[0, 7],\n",
      "       [1, 5],\n",
      "       [1, 2],\n",
      "       [2, 2],\n",
      "       [3, 4],\n",
      "       [1, 4],\n",
      "       [2, 0],\n",
      "       [1, 6]]), array([[1, 6],\n",
      "       [0, 5],\n",
      "       [0, 7],\n",
      "       [2, 7],\n",
      "       [0, 1],\n",
      "       [2, 3],\n",
      "       [3, 7],\n",
      "       [4, 1]]))\n",
      "04/22 09:06:15 PM valid 000 5.351911e-01 75.000000 100.000000\n",
      "04/22 09:06:17 PM epoch 1  |  train_acc 86.727997  |  valid_acc 81.875000\n",
      "04/22 09:06:18 PM train 000 5.303071e-01 75.000000 100.000000\n",
      "04/22 09:06:47 PM train 050 3.278678e-01 89.215691 99.754906\n",
      "04/22 09:07:13 PM train 100 3.277852e-01 89.170792 99.628708\n",
      "04/22 09:07:39 PM train 150 3.237707e-01 89.114235 99.668877\n",
      "04/22 09:08:05 PM train 200 3.112993e-01 89.521141 99.657959\n",
      "04/22 09:08:30 PM train 250 3.049895e-01 89.741035 99.676300\n",
      "04/22 09:08:58 PM train 300 3.099607e-01 89.472588 99.647011\n",
      "04/22 09:09:24 PM train 350 3.041229e-01 89.458687 99.661682\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-34ce87e00f9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'budget: %d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m     \u001b[0marchs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_eval_arch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-27ee95d15fda>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0march\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_arch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#######################need to generate architecture based on novelty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0march\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miters\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-4546af8a9b79>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, arch)\u001b[0m\n\u001b[1;32m    165\u001b[0m       \u001b[0;31m# get a random minibatch from the search queue with replacement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/anaconda3install/data/mzhang3/anaconda3install/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/randomNAS_own/searchers/model_search.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    108\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malphas_normal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m       \u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/anaconda3install/data/mzhang3/anaconda3install/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/randomNAS_own/searchers/model_search.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, s0, s1, weights)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/randomNAS_own/searchers/model_search.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m       \u001b[0moffset\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m       \u001b[0mstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/anaconda3install/data/mzhang3/anaconda3install/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/randomNAS_own/searchers/model_search.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, weights)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/randomNAS_own/searchers/model_search.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/anaconda3install/data/mzhang3/anaconda3install/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/randomNAS_own/searchers/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/anaconda3install/data/mzhang3/anaconda3install/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/anaconda3install/data/mzhang3/anaconda3install/anaconda3/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/anaconda3install/data/mzhang3/anaconda3install/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/anaconda3install/data/mzhang3/anaconda3install/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/mzhang3/anaconda3install/data/mzhang3/anaconda3install/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1622\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1623\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1624\u001b[0m     )\n\u001b[1;32m   1625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sys.argv=['']; del sys\n",
    "parser = argparse.ArgumentParser(description='Args for SHA with weight sharing')\n",
    "parser.add_argument('--benchmark', dest='benchmark', type=str, default='cnn')\n",
    "parser.add_argument('--seed', dest='seed', type=int, default=100)\n",
    "parser.add_argument('--epochs', dest='epochs', type=int, default=2)\n",
    "parser.add_argument('--batch_size', dest='batch_size', type=int, default=16)\n",
    "parser.add_argument('--grad_clip', dest='grad_clip', type=float, default=5)\n",
    "parser.add_argument('--save_dir', dest='save_dir', type=str, default='/data/mzhang3/randomNAS_release-master/results')\n",
    "parser.add_argument('--eval_only', dest='eval_only', type=int, default=0)\n",
    "# PTB only argument. config=search uses proxy network for shared weights while\n",
    "# config=eval uses proxyless network for shared weights.\n",
    "parser.add_argument('--config', dest='config', type=str, default=\"search\")\n",
    "# CIFAR-10 only argument.  Use either 16 or 24 for the settings for random search\n",
    "# with weight-sharing used in our experiments.\n",
    "parser.add_argument('--init_channels', dest='init_channels', type=int, default=16)\n",
    "args = parser.parse_args()\n",
    "import sys \n",
    "    \n",
    "# Fill in with root output path\n",
    "root_dir = '/data/mzhang3/randomNAS_release-master/results'\n",
    "if args.save_dir is None:\n",
    "    save_dir = os.path.join(root_dir, '%s/random/trial%d' % (args.benchmark, args.seed))\n",
    "else:\n",
    "    save_dir = args.save_dir\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "if args.eval_only:\n",
    "    assert args.save_dir is not None\n",
    "\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(save_dir, 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "logging.info(args)\n",
    "\n",
    "if args.benchmark=='ptb':\n",
    "    data_size = 929589\n",
    "    time_steps = 35\n",
    "else:\n",
    "    data_size = 25000\n",
    "    time_steps = 1\n",
    "B = int(args.epochs * data_size / args.batch_size / time_steps)\n",
    "model = DartsWrapper(save_dir, args.seed, args.batch_size, args.grad_clip, args.epochs, init_channels=args.init_channels)\n",
    "\n",
    "searcher = Random_NAS(B, model, args.seed, save_dir)\n",
    "logging.info('budget: %d' % (searcher.B))\n",
    "if not args.eval_only:\n",
    "    searcher.run()\n",
    "    #archs = searcher.get_eval_arch()\n",
    "    archs = searcher.EA_arch_search(num_pop=50,num_ite=50,num_cross=30,num_mutation=20)\n",
    "\n",
    "else:\n",
    "    np.random.seed(args.seed+1)\n",
    "    archs = searcher.get_eval_arch(2)\n",
    "logging.info(archs)\n",
    "arch = ' '.join([str(a) for a in archs[0][0]])\n",
    "with open('/tmp/arch','w') as f:\n",
    "    f.write(arch)\n",
    "\n",
    "       \n",
    "print(arch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
