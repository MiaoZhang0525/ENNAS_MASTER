{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment dir : search-EXP-20190702-195936\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import utils\n",
    "import logging\n",
    "import argparse\n",
    "import torch.nn as nn\n",
    "import torch.utils\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from model_search import Network\n",
    "from architect import Architect\n",
    "import genotypes\n",
    "\n",
    "parser = argparse.ArgumentParser(\"cifar\")\n",
    "parser.add_argument('--data', type=str, default='../data', help='location of the data corpus')\n",
    "parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
    "parser.add_argument('--learning_rate', type=float, default=0.025, help='init learning rate')\n",
    "parser.add_argument('--learning_rate_min', type=float, default=0.001, help='min learning rate')\n",
    "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
    "parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')\n",
    "parser.add_argument('--report_freq', type=float, default=50, help='report frequency')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
    "parser.add_argument('--epochs', type=int, default=50, help='num of training epochs')\n",
    "parser.add_argument('--init_channels', type=int, default=16, help='num of init channels')\n",
    "parser.add_argument('--layers', type=int, default=8, help='total number of layers')\n",
    "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
    "parser.add_argument('--cutout', action='store_true', default=False, help='use cutout')\n",
    "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
    "parser.add_argument('--drop_path_prob', type=float, default=0.3, help='drop path probability')\n",
    "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
    "parser.add_argument('--seed', type=int, default=2, help='random seed')\n",
    "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
    "parser.add_argument('--train_portion', type=float, default=0.5, help='portion of training data')\n",
    "parser.add_argument('--unrolled', action='store_true', default=True, help='use one-step unrolled validation loss')\n",
    "parser.add_argument('--arch_learning_rate', type=float, default=3e-4, help='learning rate for arch encoding')\n",
    "parser.add_argument('--arch_weight_decay', type=float, default=1e-3, help='weight decay for arch encoding')\n",
    "args = parser.parse_args(args=[])\n",
    "\n",
    "args.save = 'search-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
    "utils.create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
    "\n",
    "log_format = '%(asctime)s %(message)s'\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
    "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
    "fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
    "fh.setFormatter(logging.Formatter(log_format))\n",
    "logging.getLogger().addHandler(fh)\n",
    "\n",
    "\n",
    "CIFAR_CLASSES = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not torch.cuda.is_available():\n",
    "    logging.info('no gpu device available')\n",
    "    sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/02 07:59:46 PM gpu device = 0\n",
      "07/02 07:59:46 PM args = Namespace(arch_learning_rate=0.0003, arch_weight_decay=0.001, batch_size=64, cutout=False, cutout_length=16, data='../data', drop_path_prob=0.3, epochs=50, gpu=0, grad_clip=5, init_channels=16, layers=8, learning_rate=0.025, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, report_freq=50, save='search-EXP-20190702-195936', seed=2, train_portion=0.5, unrolled=True, weight_decay=0.0003)\n",
      "07/02 07:59:47 PM param size = 1.930618MB\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(args.seed)\n",
    "torch.cuda.set_device(args.gpu)\n",
    "cudnn.benchmark = True\n",
    "torch.manual_seed(args.seed)\n",
    "cudnn.enabled=True\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "logging.info('gpu device = %d' % args.gpu)\n",
    "logging.info(\"args = %s\", args)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.cuda()\n",
    "model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion)\n",
    "model = model.cuda()\n",
    "logging.info(\"param size = %fMB\", utils.count_parameters_in_MB(model))\n",
    "\n",
    "optimizer = torch.optim.SGD(\n",
    "  model.parameters(),\n",
    "  args.learning_rate,\n",
    "  momentum=args.momentum,\n",
    "  weight_decay=args.weight_decay)\n",
    "\n",
    "train_transform, valid_transform = utils._data_transforms_cifar10(args)\n",
    "train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\n",
    "\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(args.train_portion * num_train))\n",
    "\n",
    "train_queue = torch.utils.data.DataLoader(\n",
    "  train_data, batch_size=args.batch_size,\n",
    "  sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
    "  pin_memory=True, num_workers=2)\n",
    "\n",
    "valid_queue = torch.utils.data.DataLoader(\n",
    "  train_data, batch_size=args.batch_size,\n",
    "  sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
    "  pin_memory=True, num_workers=2)\n",
    "\n",
    "train_iter = iter(train_queue)\n",
    "valid_iter = iter(valid_queue)\n",
    "\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
    "\n",
    "architect = Architect(model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_queue, valid_queue, model, architect, criterion, optimizer, lr):\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    top5 = utils.AvgrageMeter()\n",
    "\n",
    "    for step, (input, target) in enumerate(train_queue):\n",
    "        model.train()\n",
    "        n = input.size(0)\n",
    "\n",
    "        input = Variable(input, requires_grad=False).cuda()\n",
    "        target = Variable(target, requires_grad=False).cuda(async=True)\n",
    "\n",
    "        # get a random minibatch from the search queue with replacement\n",
    "        input_search, target_search = next(iter(valid_queue))\n",
    "        input_search = Variable(input_search, requires_grad=False).cuda()\n",
    "        target_search = Variable(target_search, requires_grad=False).cuda(async=True)\n",
    "\n",
    "        architect.step(input, target, input_search, target_search, lr, optimizer, unrolled=args.unrolled)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(input)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        objs.update(loss.data, n)\n",
    "        top1.update(prec1.data, n)\n",
    "        top5.update(prec5.data, n)\n",
    "\n",
    "        if step % args.report_freq == 0:\n",
    "            logging.info('train %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
    "\n",
    "    return top1.avg, objs.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01 03:02:36 PM epoch 1 lr 2.490538e-02\n",
      "07/01 03:02:36 PM genotype = Genotype(normal=[('avg_pool_3x3', 0), ('dil_conv_5x5', 1), ('dil_conv_3x3', 1), ('dil_conv_5x5', 2), ('max_pool_3x3', 1), ('avg_pool_3x3', 0), ('dil_conv_5x5', 1), ('avg_pool_3x3', 0)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 1), ('avg_pool_3x3', 0), ('sep_conv_3x3', 1), ('dil_conv_5x5', 2), ('sep_conv_3x3', 2), ('avg_pool_3x3', 3), ('max_pool_3x3', 4), ('dil_conv_5x5', 0)], reduce_concat=range(2, 6))\n",
      "tensor([[0.1249, 0.1249, 0.1252, 0.1251, 0.1250, 0.1250, 0.1250, 0.1249],\n",
      "        [0.1250, 0.1248, 0.1251, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251],\n",
      "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249],\n",
      "        [0.1249, 0.1249, 0.1249, 0.1250, 0.1249, 0.1250, 0.1253, 0.1251],\n",
      "        [0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249, 0.1251],\n",
      "        [0.1250, 0.1250, 0.1252, 0.1251, 0.1249, 0.1249, 0.1250, 0.1249],\n",
      "        [0.1249, 0.1253, 0.1250, 0.1248, 0.1248, 0.1251, 0.1251, 0.1250],\n",
      "        [0.1249, 0.1251, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1249],\n",
      "        [0.1251, 0.1250, 0.1250, 0.1250, 0.1249, 0.1251, 0.1249, 0.1250],\n",
      "        [0.1249, 0.1248, 0.1252, 0.1247, 0.1251, 0.1249, 0.1252, 0.1251],\n",
      "        [0.1249, 0.1249, 0.1251, 0.1250, 0.1248, 0.1250, 0.1249, 0.1254],\n",
      "        [0.1251, 0.1250, 0.1250, 0.1250, 0.1252, 0.1250, 0.1249, 0.1250],\n",
      "        [0.1251, 0.1249, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1251],\n",
      "        [0.1251, 0.1252, 0.1251, 0.1247, 0.1252, 0.1249, 0.1249, 0.1250]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.1251, 0.1251, 0.1251, 0.1250, 0.1247, 0.1250, 0.1251, 0.1250],\n",
      "        [0.1250, 0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1251, 0.1248],\n",
      "        [0.1252, 0.1251, 0.1250, 0.1250, 0.1249, 0.1249, 0.1250, 0.1250],\n",
      "        [0.1248, 0.1249, 0.1250, 0.1249, 0.1252, 0.1250, 0.1251, 0.1251],\n",
      "        [0.1252, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1251],\n",
      "        [0.1249, 0.1251, 0.1250, 0.1250, 0.1250, 0.1251, 0.1250, 0.1249],\n",
      "        [0.1249, 0.1249, 0.1251, 0.1251, 0.1246, 0.1251, 0.1251, 0.1251],\n",
      "        [0.1250, 0.1247, 0.1250, 0.1251, 0.1252, 0.1250, 0.1250, 0.1250],\n",
      "        [0.1252, 0.1249, 0.1252, 0.1247, 0.1249, 0.1251, 0.1250, 0.1250],\n",
      "        [0.1248, 0.1251, 0.1251, 0.1249, 0.1249, 0.1249, 0.1251, 0.1252],\n",
      "        [0.1249, 0.1250, 0.1250, 0.1251, 0.1251, 0.1251, 0.1249, 0.1249],\n",
      "        [0.1250, 0.1249, 0.1249, 0.1252, 0.1250, 0.1250, 0.1251, 0.1250],\n",
      "        [0.1249, 0.1251, 0.1249, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250],\n",
      "        [0.1251, 0.1253, 0.1249, 0.1250, 0.1248, 0.1249, 0.1248, 0.1251]],\n",
      "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    scheduler.step()\n",
    "    lr = scheduler.get_lr()[0]\n",
    "    logging.info('epoch %d lr %e', epoch, lr)\n",
    "\n",
    "    genotype = model.genotype()\n",
    "    logging.info('genotype = %s', genotype)\n",
    "    \n",
    "    train_acc, train_obj = train(train_queue, valid_queue, model, architect, criterion, optimizer, lr)\n",
    "    utils.save(model, os.path.join(args.save, 'weights.pt'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01 03:03:45 PM train 000 2.484944e+00 18.750000 43.750000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mzhang3/anaconda3install/data/mzhang3/anaconda3install/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01 03:07:58 PM train 050 2.371466e+00 19.362745 69.730392\n",
      "07/01 03:12:13 PM train 100 2.212683e+00 21.782177 74.195541\n",
      "07/01 03:16:25 PM train 150 2.151035e+00 22.392384 76.117546\n",
      "07/01 03:20:39 PM train 200 2.076057e+00 24.347013 78.451492\n",
      "07/01 03:24:52 PM train 250 2.044914e+00 24.327690 79.830681\n",
      "07/01 03:29:05 PM train 300 2.015373e+00 25.373755 80.398666\n",
      "07/01 03:33:17 PM train 350 1.986691e+00 26.371082 81.303421\n",
      "07/01 03:37:29 PM train 400 1.957622e+00 27.462595 81.857857\n",
      "07/01 03:41:42 PM train 450 1.940889e+00 27.896341 82.026054\n",
      "07/01 03:45:56 PM train 500 1.922935e+00 28.468063 82.435127\n",
      "07/01 03:50:09 PM train 550 1.898903e+00 29.378403 83.076225\n",
      "07/01 03:54:23 PM train 600 1.880595e+00 30.147669 83.381859\n",
      "07/01 03:58:37 PM train 650 1.868862e+00 30.741167 83.563751\n",
      "07/01 04:02:50 PM train 700 1.851412e+00 31.339159 83.978249\n",
      "07/01 04:07:04 PM train 750 1.838183e+00 31.849199 84.287613\n",
      "07/01 04:11:18 PM train 800 1.821393e+00 32.529652 84.683205\n",
      "07/01 04:15:30 PM train 850 1.810875e+00 32.821678 84.951523\n",
      "07/01 04:19:44 PM train 900 1.800666e+00 33.150665 85.238625\n",
      "07/01 04:23:57 PM train 950 1.790121e+00 33.583069 85.469238\n",
      "07/01 04:28:10 PM train 1000 1.778353e+00 34.184563 85.689308\n",
      "07/01 04:32:23 PM train 1050 1.767462e+00 34.722881 85.918175\n",
      "07/01 04:36:36 PM train 1100 1.753087e+00 35.382607 86.131927\n",
      "07/01 04:40:48 PM train 1150 1.742428e+00 35.908993 86.337967\n",
      "07/01 04:44:59 PM train 1200 1.731215e+00 36.407162 86.636139\n",
      "07/01 04:49:10 PM train 1250 1.720971e+00 36.820545 86.855515\n",
      "07/01 04:53:22 PM train 1300 1.711495e+00 37.202152 87.019600\n",
      "07/01 04:57:34 PM train 1350 1.701301e+00 37.560139 87.259438\n",
      "07/01 05:01:47 PM train 1400 1.689091e+00 38.155781 87.459846\n",
      "07/01 05:06:00 PM train 1450 1.680077e+00 38.533768 87.637833\n",
      "07/01 05:10:13 PM train 1500 1.671763e+00 38.849102 87.787308\n",
      "07/01 05:14:26 PM train 1550 1.661929e+00 39.103802 87.979530\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weights_from_arch(model, arch):\n",
    "    k = sum(1 for i in range(model._steps) for n in range(2+i))\n",
    "    num_ops = len(genotypes.PRIMITIVES)\n",
    "    n_nodes = model._steps\n",
    "\n",
    "    alphas_normal = Variable(torch.zeros(k, num_ops).cuda(), requires_grad=False)\n",
    "    alphas_reduce = Variable(torch.zeros(k, num_ops).cuda(), requires_grad=False)\n",
    "\n",
    "    offset = 0\n",
    "    for i in range(n_nodes):\n",
    "        normal1 = arch[0][2*i]\n",
    "        normal2 = arch[0][2*i+1]\n",
    "        reduce1 = arch[1][2*i]\n",
    "        reduce2 = arch[1][2*i+1]\n",
    "        alphas_normal[offset+normal1[0], normal1[1]] = 1\n",
    "        alphas_normal[offset+normal2[0], normal2[1]] = 1\n",
    "        alphas_reduce[offset+reduce1[0], reduce1[1]] = 1\n",
    "        alphas_reduce[offset+reduce2[0], reduce2[1]] = 1\n",
    "        offset += (i+2)\n",
    "\n",
    "    arch_parameters = [\n",
    "      alphas_normal,\n",
    "      alphas_reduce,\n",
    "    ]\n",
    "    return arch_parameters\n",
    "\n",
    "\n",
    "def set_model_weights(model, weights):\n",
    "    model.alphas_normal = weights[0]\n",
    "    model.alphas_reduce = weights[1]\n",
    "    model._arch_parameters = [model.alphas_normal, model.alphas_reduce]\n",
    "    \n",
    "def random_sample_arch(model):\n",
    "    k = sum(1 for i in range(model._steps) for n in range(2+i))\n",
    "    num_ops = len(genotypes.PRIMITIVES)\n",
    "    n_nodes = model._steps\n",
    "\n",
    "    normal = []\n",
    "    reduction = []\n",
    "    for i in range(n_nodes):\n",
    "        ops = np.random.choice(range(num_ops), 4)\n",
    "        nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)\n",
    "        nodes_in_reduce = np.random.choice(range(i+2), 2, replace=False)\n",
    "        normal.extend([(nodes_in_normal[0], ops[0]), (nodes_in_normal[1], ops[1])])\n",
    "        reduction.extend([(nodes_in_reduce[0], ops[2]), (nodes_in_reduce[1], ops[3])])\n",
    "    return (normal, reduction)        \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model,arch,valid_queue,valid_iter):\n",
    "  # Return error since we want to minimize obj val\n",
    "    logging.info(arch)\n",
    "    objs = utils.AvgrageMeter()\n",
    "    top1 = utils.AvgrageMeter()\n",
    "    top5 = utils.AvgrageMeter()\n",
    "\n",
    "    weights = get_weights_from_arch(model,arch)\n",
    "    set_model_weights(model,weights)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    n_batches = len(valid_queue)\n",
    "\n",
    "    for step in range(n_batches):\n",
    "        try:\n",
    "            input, target = next(valid_iter)\n",
    "        except Exception as e:\n",
    "            logging.info('looping back over valid set')\n",
    "            valid_iter = iter(valid_queue)\n",
    "            input, target = next(valid_iter)\n",
    "        input = Variable(input).cuda()\n",
    "        target = Variable(target).cuda(async=True)\n",
    "\n",
    "        logits = model(input)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))\n",
    "        n = input.size(0)\n",
    "        objs.update(loss.data, n)\n",
    "        top1.update(prec1.data, n)\n",
    "        top5.update(prec5.data, n)\n",
    "\n",
    "\n",
    "    return 1-(top1.avg)/100\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#data_size = 929589\n",
    "#ime_steps = 35\n",
    "data_size = 25000\n",
    "time_steps = 1\n",
    "B = int(args.epochs * data_size / args.batch_size / time_steps)   \n",
    "\n",
    "def random_selection(model,valid_queue,valid_iter, rounds=None):\n",
    "    #n_rounds = int(self.B / 7 / 1000)\n",
    "    if rounds is None:\n",
    "        n_rounds = max(1,int(B/10000))\n",
    "    else:\n",
    "        n_rounds = rounds\n",
    "    best_rounds = []\n",
    "    for r in range(n_rounds):\n",
    "        sample_vals = []\n",
    "        for _ in range(1000):\n",
    "            arch = random_sample_arch(model)\n",
    "            try:\n",
    "                ppl = evaluate(model,arch,valid_queue,valid_iter)\n",
    "            except Exception as e:\n",
    "                ppl = 1000000\n",
    "            logging.info(arch)\n",
    "            logging.info('objective_val: %.3f' % ppl)\n",
    "            sample_vals.append((arch, ppl))\n",
    "        sample_vals = sorted(sample_vals, key=lambda x:x[1])\n",
    "\n",
    "        full_vals = []\n",
    "        best_rounds.append(sample_vals[0])\n",
    "    return best_rounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_init_pop(model,num_pop,n_nodes,valid_queue,valid_iter):\n",
    "    pop=np.empty((num_pop,8*n_nodes))\n",
    "    fitness=np.zeros((num_pop,))\n",
    "    for m in range(num_pop):         \n",
    "        k = sum(1 for i in range(model._steps) for n in range(2+i))\n",
    "        num_ops = len(genotypes.PRIMITIVES)\n",
    "        normal = []\n",
    "        reduction = []\n",
    "        for i in range(n_nodes):\n",
    "            ops = np.random.choice(range(num_ops), 4)\n",
    "            nodes_in_normal = np.random.choice(range(i+2), 2, replace=False)\n",
    "            nodes_in_reduce = np.random.choice(range(i+2), 2, replace=False)\n",
    "            normal.extend([(nodes_in_normal[0], ops[0]), (nodes_in_normal[1], ops[1])])\n",
    "            reduction.extend([(nodes_in_reduce[0], ops[2]), (nodes_in_reduce[1], ops[3])])\n",
    "            pop[m,4*i]=nodes_in_normal[0]\n",
    "            pop[m,4*i+1]=ops[0]\n",
    "            pop[m,4*i+2]=nodes_in_normal[1]\n",
    "            pop[m,4*i+3]=ops[1]\n",
    "            pop[m,4*i+4*n_nodes]=nodes_in_reduce[0]\n",
    "            pop[m,4*i+1+4*n_nodes]=ops[2]\n",
    "            pop[m,4*i+2+4*n_nodes]=nodes_in_reduce[1]\n",
    "            pop[m,4*i+3+4*n_nodes]=ops[3]                            \n",
    "        arch=(normal, reduction) \n",
    "        fitness[m,]=evaluate(model,arch,valid_queue,valid_iter)\n",
    "    return pop,fitness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01 08:05:40 PM ([(1, 6), (0, 3), (2, 2), (1, 3), (0, 5), (3, 0), (0, 2), (1, 7)], [(1, 4), (0, 5), (0, 3), (2, 0), (0, 0), (3, 6), (3, 3), (4, 7)])\n",
      "07/01 08:05:40 PM looping back over valid set\n",
      "07/01 08:09:05 PM ([(1, 6), (0, 3), (2, 2), (1, 3), (0, 5), (3, 0), (0, 2), (1, 7)], [(1, 4), (0, 5), (0, 3), (2, 0), (0, 0), (3, 6), (3, 3), (4, 7)])\n",
      "07/01 08:09:05 PM objective_val: 0.510\n",
      "07/01 08:09:05 PM ([(1, 3), (0, 0), (1, 4), (0, 4), (1, 1), (0, 1), (1, 4), (0, 6)], [(0, 6), (1, 0), (0, 7), (2, 3), (2, 2), (0, 1), (2, 3), (1, 1)])\n",
      "07/01 08:09:05 PM looping back over valid set\n",
      "07/01 08:12:33 PM ([(1, 3), (0, 0), (1, 4), (0, 4), (1, 1), (0, 1), (1, 4), (0, 6)], [(0, 6), (1, 0), (0, 7), (2, 3), (2, 2), (0, 1), (2, 3), (1, 1)])\n",
      "07/01 08:12:33 PM objective_val: 0.516\n",
      "07/01 08:12:33 PM ([(1, 3), (0, 2), (2, 7), (0, 3), (2, 7), (3, 3), (1, 5), (3, 1)], [(0, 2), (1, 5), (1, 5), (0, 4), (3, 1), (1, 3), (0, 7), (2, 2)])\n",
      "07/01 08:12:33 PM looping back over valid set\n",
      "07/01 08:15:56 PM ([(1, 3), (0, 2), (2, 7), (0, 3), (2, 7), (3, 3), (1, 5), (3, 1)], [(0, 2), (1, 5), (1, 5), (0, 4), (3, 1), (1, 3), (0, 7), (2, 2)])\n",
      "07/01 08:15:56 PM objective_val: 0.573\n",
      "07/01 08:15:56 PM ([(1, 0), (0, 5), (1, 7), (0, 0), (0, 3), (3, 0), (3, 5), (1, 5)], [(1, 1), (0, 5), (1, 7), (2, 3), (1, 0), (0, 4), (2, 2), (3, 5)])\n",
      "07/01 08:15:56 PM looping back over valid set\n",
      "07/01 08:19:19 PM ([(1, 0), (0, 5), (1, 7), (0, 0), (0, 3), (3, 0), (3, 5), (1, 5)], [(1, 1), (0, 5), (1, 7), (2, 3), (1, 0), (0, 4), (2, 2), (3, 5)])\n",
      "07/01 08:19:19 PM objective_val: 0.471\n",
      "07/01 08:19:19 PM ([(0, 4), (1, 0), (2, 5), (0, 1), (0, 6), (3, 0), (1, 5), (3, 7)], [(0, 6), (1, 2), (0, 5), (1, 4), (2, 5), (3, 7), (4, 5), (3, 4)])\n",
      "07/01 08:19:19 PM looping back over valid set\n",
      "07/01 08:22:42 PM ([(0, 4), (1, 0), (2, 5), (0, 1), (0, 6), (3, 0), (1, 5), (3, 7)], [(0, 6), (1, 2), (0, 5), (1, 4), (2, 5), (3, 7), (4, 5), (3, 4)])\n",
      "07/01 08:22:42 PM objective_val: 0.475\n",
      "07/01 08:22:42 PM ([(0, 1), (1, 5), (1, 1), (2, 7), (2, 2), (1, 4), (2, 3), (0, 5)], [(1, 1), (0, 5), (1, 0), (2, 3), (0, 3), (3, 7), (1, 4), (0, 7)])\n",
      "07/01 08:22:42 PM looping back over valid set\n",
      "07/01 08:26:08 PM ([(0, 1), (1, 5), (1, 1), (2, 7), (2, 2), (1, 4), (2, 3), (0, 5)], [(1, 1), (0, 5), (1, 0), (2, 3), (0, 3), (3, 7), (1, 4), (0, 7)])\n",
      "07/01 08:26:08 PM objective_val: 0.525\n",
      "07/01 08:26:08 PM ([(0, 2), (1, 2), (2, 3), (0, 1), (2, 5), (0, 2), (3, 4), (0, 0)], [(1, 6), (0, 5), (0, 6), (1, 0), (0, 2), (1, 0), (0, 4), (2, 7)])\n",
      "07/01 08:26:08 PM looping back over valid set\n",
      "07/01 08:29:31 PM ([(0, 2), (1, 2), (2, 3), (0, 1), (2, 5), (0, 2), (3, 4), (0, 0)], [(1, 6), (0, 5), (0, 6), (1, 0), (0, 2), (1, 0), (0, 4), (2, 7)])\n",
      "07/01 08:29:31 PM objective_val: 0.529\n",
      "07/01 08:29:31 PM ([(1, 1), (0, 1), (2, 0), (0, 2), (3, 6), (1, 7), (2, 2), (0, 6)], [(1, 1), (0, 3), (0, 0), (1, 5), (0, 6), (3, 1), (2, 6), (0, 3)])\n",
      "07/01 08:29:31 PM looping back over valid set\n",
      "07/01 08:32:54 PM ([(1, 1), (0, 1), (2, 0), (0, 2), (3, 6), (1, 7), (2, 2), (0, 6)], [(1, 1), (0, 3), (0, 0), (1, 5), (0, 6), (3, 1), (2, 6), (0, 3)])\n",
      "07/01 08:32:54 PM objective_val: 0.531\n",
      "07/01 08:32:54 PM ([(0, 4), (1, 4), (1, 4), (0, 2), (1, 5), (0, 0), (3, 7), (4, 6)], [(1, 2), (0, 7), (0, 2), (1, 3), (0, 4), (3, 5), (4, 2), (2, 6)])\n",
      "07/01 08:32:54 PM looping back over valid set\n",
      "07/01 08:36:20 PM ([(0, 4), (1, 4), (1, 4), (0, 2), (1, 5), (0, 0), (3, 7), (4, 6)], [(1, 2), (0, 7), (0, 2), (1, 3), (0, 4), (3, 5), (4, 2), (2, 6)])\n",
      "07/01 08:36:20 PM objective_val: 0.490\n",
      "07/01 08:36:20 PM ([(0, 4), (1, 2), (2, 3), (0, 4), (0, 4), (1, 6), (0, 1), (3, 7)], [(0, 3), (1, 7), (0, 3), (1, 1), (3, 5), (0, 6), (1, 5), (3, 6)])\n",
      "07/01 08:36:20 PM looping back over valid set\n",
      "07/01 08:39:45 PM ([(0, 4), (1, 2), (2, 3), (0, 4), (0, 4), (1, 6), (0, 1), (3, 7)], [(0, 3), (1, 7), (0, 3), (1, 1), (3, 5), (0, 6), (1, 5), (3, 6)])\n",
      "07/01 08:39:45 PM objective_val: 0.507\n",
      "07/01 08:39:45 PM ([(1, 3), (0, 0), (1, 3), (2, 3), (2, 6), (0, 6), (2, 3), (1, 7)], [(1, 6), (0, 2), (2, 7), (0, 7), (1, 2), (3, 2), (3, 3), (2, 0)])\n",
      "07/01 08:39:45 PM looping back over valid set\n",
      "07/01 08:43:09 PM ([(1, 3), (0, 0), (1, 3), (2, 3), (2, 6), (0, 6), (2, 3), (1, 7)], [(1, 6), (0, 2), (2, 7), (0, 7), (1, 2), (3, 2), (3, 3), (2, 0)])\n",
      "07/01 08:43:09 PM objective_val: 0.545\n",
      "07/01 08:43:09 PM ([(0, 5), (1, 7), (1, 0), (2, 6), (1, 6), (0, 7), (4, 7), (2, 6)], [(0, 4), (1, 1), (2, 6), (0, 7), (1, 5), (2, 1), (3, 2), (4, 2)])\n",
      "07/01 08:43:09 PM looping back over valid set\n",
      "07/01 08:46:33 PM ([(0, 5), (1, 7), (1, 0), (2, 6), (1, 6), (0, 7), (4, 7), (2, 6)], [(0, 4), (1, 1), (2, 6), (0, 7), (1, 5), (2, 1), (3, 2), (4, 2)])\n",
      "07/01 08:46:33 PM objective_val: 0.471\n",
      "07/01 08:46:33 PM ([(1, 5), (0, 0), (0, 6), (2, 4), (3, 2), (0, 4), (2, 6), (4, 6)], [(1, 1), (0, 0), (2, 3), (0, 1), (1, 4), (0, 1), (1, 7), (2, 1)])\n",
      "07/01 08:46:33 PM looping back over valid set\n",
      "07/01 08:49:57 PM ([(1, 5), (0, 0), (0, 6), (2, 4), (3, 2), (0, 4), (2, 6), (4, 6)], [(1, 1), (0, 0), (2, 3), (0, 1), (1, 4), (0, 1), (1, 7), (2, 1)])\n",
      "07/01 08:49:57 PM objective_val: 0.471\n",
      "07/01 08:49:57 PM ([(0, 3), (1, 4), (2, 1), (0, 2), (1, 3), (3, 3), (4, 7), (2, 3)], [(0, 4), (1, 7), (2, 6), (1, 2), (2, 6), (3, 0), (3, 5), (4, 5)])\n",
      "07/01 08:49:57 PM looping back over valid set\n",
      "07/01 08:53:18 PM ([(0, 3), (1, 4), (2, 1), (0, 2), (1, 3), (3, 3), (4, 7), (2, 3)], [(0, 4), (1, 7), (2, 6), (1, 2), (2, 6), (3, 0), (3, 5), (4, 5)])\n",
      "07/01 08:53:18 PM objective_val: 0.540\n"
     ]
    }
   ],
   "source": [
    "n_nodes = model._steps    \n",
    "archs =random_selection(model,valid_queue,valid_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corssover(model,pop,fitness,num_cross):\n",
    "    index=np.argsort(fitness)\n",
    "    pop_select=pop[index[0:num_cross],]\n",
    "\n",
    "\n",
    "    inde_cross=np.arange(num_cross)\n",
    "    np.random.shuffle(inde_cross)\n",
    "    pop_select=pop_select[inde_cross,]\n",
    "    pop_cross=np.empty((num_cross,pop.shape[1]))\n",
    "\n",
    "\n",
    "    for i in range(np.int(num_cross/2)):\n",
    "        cross1=pop_select[2*i,]\n",
    "        cross2=pop_select[2*i+1,]\n",
    "\n",
    "        cross_points=np.arange(4*model._steps)\n",
    "        np.random.shuffle(cross_points)\n",
    "        cross_points=cross_points[0:2]\n",
    "        cross_points=np.sort(cross_points)\n",
    "        p1=2*cross_points[0]\n",
    "        p2=2*cross_points[1]\n",
    "\n",
    "        cross1_=cross1\n",
    "        cross2_=cross2\n",
    "\n",
    "        cross1_[p1:p2]=cross2[p1:p2]\n",
    "        cross2_[p1:p2]=cross1[p1:p2]\n",
    "\n",
    "        pop_cross[2*i,]= cross1_       \n",
    "        pop_cross[2*i+1,]= cross2_   \n",
    "\n",
    "    return pop_cross\n",
    "\n",
    "def mutation(model,pop,fitness,num_mutation):\n",
    "    index=np.argsort(fitness)\n",
    "    pop_select=pop[index[0:num_mutation],]\n",
    "    pop_mutation=np.empty((num_mutation,pop.shape[1]))\n",
    "    num_ops = len(genotypes.PRIMITIVES)\n",
    "\n",
    "\n",
    "    for i in range(num_mutation):\n",
    "        pop_mutation[i,]=pop_select[i,]\n",
    "\n",
    "        for j in range(pop.shape[1]):\n",
    "            if j>((pop.shape[1])/2-1):\n",
    "                q=j-(pop.shape[1])/2\n",
    "            else:\n",
    "                q=j\n",
    "            m=q//4+2\n",
    "            if np.random.rand()<0.2:#################genes with mutation probability 0.2\n",
    "                if j%2==0:\n",
    "                    pop_mutation[i,j]=np.random.randint(m)\n",
    "                else:\n",
    "                    pop_mutation[i,j]=np.random.randint(num_ops)            \n",
    "    return pop_mutation\n",
    "\n",
    "\n",
    "def get_fitness(model,pop):\n",
    "    num_pop=pop.shape[0]\n",
    "    fitness=np.zeros((num_pop))\n",
    "    for m in range(num_pop):\n",
    "        indiv=pop[m,]\n",
    "        normal=[]\n",
    "        reduction=[]\n",
    "        for i in range(model._steps):\n",
    "            s1=np.int(indiv[4*i,])\n",
    "            s2=np.int(indiv[4*i+1,])\n",
    "            s3=np.int(indiv[4*i+2,])\n",
    "            s4=np.int(indiv[4*i+3,])\n",
    "            s5=np.int(indiv[4*i+16,])\n",
    "            s6=np.int(indiv[4*i+1+16,])\n",
    "            s7=np.int(indiv[4*i+2+16,])\n",
    "            s8=np.int(indiv[4*i+3+16,])\n",
    "            normal.extend([(s1,s2), (s3,s4)])\n",
    "            reduction.extend([(s5,s6), (s7,s8)]) \n",
    "        arch=(normal, reduction)\n",
    "        fitness[m,]=evaluate(model,arch,valid_queue,valid_iter) \n",
    "\n",
    "    return fitness\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EA_arch_search(model,num_pop,num_ite,num_cross,num_mutation):\n",
    "    k = sum(1 for i in range(model._steps) for n in range(2+i))\n",
    "    num_ops = len(genotypes.PRIMITIVES)\n",
    "    n_nodes = model._steps    \n",
    "\n",
    "    pop,fitness=get_init_pop(model,num_pop,n_nodes,valid_queue,valid_iter)\n",
    "\n",
    "    for it in range(num_ite):\n",
    "        pop_cross=corssover(model,pop,fitness,num_cross)\n",
    "        fitness_cross=get_fitness(model,pop_cross)\n",
    "        pop_mutate=mutation(model,pop,fitness,num_mutation)\n",
    "        fitness_mutate=get_fitness(model,pop_mutate) \n",
    "        pop_comb=np.concatenate((pop,pop_cross,pop_mutate),axis=0)\n",
    "        fitness_comb=np.concatenate((fitness,fitness_cross,fitness_mutate),axis=0)\n",
    "        index=np.argsort(fitness_comb)\n",
    "        pop_comb=pop_comb[index,]\n",
    "        pop=pop_comb[0:num_pop,]\n",
    "        fitness=fitness_comb[0:num_pop,]\n",
    "\n",
    "    index=np.argsort(fitness)\n",
    "    indi_final=pop[index[0],]\n",
    "\n",
    "    normal = []\n",
    "    normal_struc=[]\n",
    "    reduction = []\n",
    "    reduction_struc=[]\n",
    "    for i in range(model._steps):\n",
    "\n",
    "        s1=np.int(indi_final[4*i,])\n",
    "        s2=np.int(indi_final[4*i+1,])\n",
    "        s3=np.int(indi_final[4*i+2,])\n",
    "        s4=np.int(indi_final[4*i+3,])\n",
    "        s5=np.int(indi_final[4*i+16,])\n",
    "        s6=np.int(indi_final[4*i+1+16,])\n",
    "        s7=np.int(indi_final[4*i+2+16,])\n",
    "        s8=np.int(indi_final[4*i+3+16,])\n",
    "        normal.extend([(s1,s2), (s3,s4)])\n",
    "        normal_struc.append((genotypes.PRIMITIVES[s1], s2))\n",
    "        normal_struc.append((genotypes.PRIMITIVES[s3], s4))\n",
    "\n",
    "        reduction.extend([(s5,s6), (s7,s8)])            \n",
    "        reduction_struc.append((genotypes.PRIMITIVES[s5], s6))\n",
    "        reduction_struc.append((genotypes.PRIMITIVES[s7], s8))\n",
    "\n",
    "    concat = range(2, model._steps+2)\n",
    "    genotype = genotypes.Genotype(normal=normal_struc, normal_concat=concat,reduce=reduction_struc, reduce_concat=concat)\n",
    "    best_arch=genotype\n",
    "\n",
    "    return best_arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/01 09:59:06 PM ([(1, 0), (0, 3), (0, 1), (1, 4), (1, 4), (0, 5), (0, 0), (4, 1)], [(1, 2), (0, 7), (0, 7), (2, 4), (3, 1), (0, 2), (2, 0), (0, 2)])\n",
      "07/01 09:59:06 PM looping back over valid set\n",
      "07/01 10:02:30 PM ([(0, 2), (1, 2), (2, 2), (0, 1), (2, 2), (3, 2), (0, 2), (2, 5)], [(1, 0), (0, 2), (0, 6), (2, 6), (3, 3), (1, 2), (4, 7), (0, 0)])\n",
      "07/01 10:02:30 PM looping back over valid set\n",
      "07/01 10:05:53 PM ([(0, 2), (1, 1), (2, 5), (0, 6), (3, 4), (0, 1), (4, 3), (2, 3)], [(1, 7), (0, 6), (0, 5), (2, 0), (0, 3), (2, 5), (3, 4), (4, 4)])\n",
      "07/01 10:05:53 PM looping back over valid set\n",
      "07/01 10:09:15 PM ([(0, 0), (1, 5), (0, 5), (2, 4), (3, 0), (1, 6), (0, 5), (1, 2)], [(1, 5), (0, 3), (1, 1), (2, 0), (3, 1), (1, 1), (1, 6), (2, 2)])\n",
      "07/01 10:09:15 PM looping back over valid set\n",
      "07/01 10:12:36 PM ([(1, 0), (0, 3), (0, 1), (1, 4), (1, 4), (0, 5), (0, 0), (4, 1)], [(1, 5), (0, 7), (0, 7), (2, 4), (3, 1), (0, 2), (2, 0), (0, 2)])\n",
      "07/01 10:12:36 PM looping back over valid set\n",
      "07/01 10:15:58 PM ([(0, 0), (1, 5), (0, 5), (2, 4), (3, 0), (1, 6), (0, 5), (1, 2)], [(1, 5), (0, 3), (1, 1), (2, 0), (3, 1), (1, 1), (1, 6), (2, 2)])\n",
      "07/01 10:15:58 PM looping back over valid set\n",
      "07/01 10:19:19 PM ([(0, 0), (1, 5), (0, 5), (2, 4), (3, 0), (1, 6), (3, 5), (1, 2)], [(0, 0), (0, 3), (1, 1), (2, 0), (3, 1), (0, 1), (1, 6), (3, 5)])\n",
      "07/01 10:19:19 PM looping back over valid set\n",
      "07/01 10:22:40 PM ([(1, 0), (0, 3), (0, 1), (1, 4), (1, 4), (2, 5), (0, 0), (4, 1)], [(1, 2), (0, 5), (0, 4), (2, 7), (2, 1), (0, 2), (2, 0), (4, 2)])\n",
      "07/01 10:22:40 PM looping back over valid set\n",
      "07/01 10:26:01 PM ([(1, 0), (0, 3), (0, 1), (1, 4), (1, 4), (0, 5), (0, 0), (4, 1)], [(1, 5), (0, 3), (1, 1), (2, 0), (3, 1), (0, 1), (1, 6), (3, 5)])\n",
      "07/01 10:26:01 PM looping back over valid set\n",
      "07/01 10:29:22 PM ([(1, 0), (0, 3), (0, 1), (1, 4), (1, 4), (0, 5), (0, 0), (4, 1)], [(1, 5), (0, 7), (0, 7), (2, 4), (3, 1), (0, 2), (2, 0), (0, 2)])\n",
      "07/01 10:29:22 PM looping back over valid set\n",
      "07/01 10:32:44 PM ([(1, 0), (0, 3), (0, 1), (1, 4), (1, 4), (0, 5), (0, 0), (4, 1)], [(1, 5), (0, 7), (0, 3), (2, 4), (3, 1), (0, 2), (2, 4), (0, 2)])\n",
      "07/01 10:32:44 PM looping back over valid set\n",
      "07/01 10:36:04 PM ([(0, 0), (1, 6), (0, 5), (2, 4), (2, 0), (1, 2), (3, 5), (1, 2)], [(0, 0), (0, 3), (1, 1), (2, 4), (2, 1), (0, 5), (1, 6), (3, 5)])\n",
      "07/01 10:36:04 PM looping back over valid set\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6f8533414f98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0marchs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEA_arch_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_pop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_ite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_cross\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_mutation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-55c976dd929d>\u001b[0m in \u001b[0;36mEA_arch_search\u001b[0;34m(model, num_pop, num_ite, num_cross, num_mutation)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mreduction_struc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenotypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRIMITIVES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0mgenotype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenotypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenotype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnormal_struc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormal_concat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduction_struc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_concat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mbest_arch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenotype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "archs = EA_arch_search(model,num_pop=4,num_ite=2,num_cross=2,num_mutation=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
